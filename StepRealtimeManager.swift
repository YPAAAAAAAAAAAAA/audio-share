import Foundation
import AVFoundation

// éŸ³é¢‘ç±»å‹æšä¸¾
enum AudioType: String, CaseIterable {
    case humanVoice = "human_voice"      // äººå£°
    case music = "music"                 // éŸ³ä¹
    case nature = "nature"               // è‡ªç„¶ç¯å¢ƒéŸ³
    case mechanical = "mechanical"        // æœºæ¢°å£°
    case conversation = "conversation"    // å¯¹è¯
    case singing = "singing"             // æ­Œå£°
    case noise = "noise"                 // å™ªéŸ³
    case unknown = "unknown"             // æœªçŸ¥
    
    var displayName: String {
        switch self {
        case .humanVoice: return "äººå£°"
        case .music: return "éŸ³ä¹"
        case .nature: return "è‡ªç„¶éŸ³"
        case .mechanical: return "æœºæ¢°éŸ³"
        case .conversation: return "å¯¹è¯"
        case .singing: return "æ­Œå£°"
        case .noise: return "å™ªéŸ³"
        case .unknown: return "æœªçŸ¥"
        }
    }
    
    var icon: String {
        switch self {
        case .humanVoice: return "person.wave.2"
        case .music: return "music.note"
        case .nature: return "leaf"
        case .mechanical: return "gearshape"
        case .conversation: return "bubble.left.and.bubble.right"
        case .singing: return "music.mic"
        case .noise: return "waveform"
        case .unknown: return "questionmark"
        }
    }
    
    var color: UIColor {
        switch self {
        case .humanVoice: return .systemBlue
        case .music: return .systemPurple
        case .nature: return .systemGreen
        case .mechanical: return .systemOrange
        case .conversation: return .systemTeal
        case .singing: return .systemPink
        case .noise: return .systemRed
        case .unknown: return .systemGray
        }
    }
}

// Step Realtime API WebSocketå®¢æˆ·ç«¯
class StepRealtimeManager: NSObject, ObservableObject {
    
    // APIé…ç½®
    private let apiKey = "8FyDGELcpTdfh1JNOoePkfXzCtExQHL8DSdEX9UYfl4dCsE77R4WIUOIJqanw0Cl"
    private let wsURL = "wss://api.stepfun.com/v1/realtime"
    
    // WebSocketè¿æ¥
    private var webSocketTask: URLSessionWebSocketTask?
    private var urlSession: URLSession!
    
    // éŸ³é¢‘å¤„ç†
    private var audioEngine = AVAudioEngine()
    private var inputNode: AVAudioInputNode!
    
    // çŠ¶æ€ç®¡ç†
    @Published var isConnected = false
    @Published var isRecording = false
    @Published var currentTranscription = ""
    @Published var currentSummary = ""
    @Published var audioType: AudioType = .unknown
    @Published var conversationHistory: [ConversationItem] = []
    
    // å®æ—¶å¤„ç†çŠ¶æ€
    @Published var processingStartTime: Date?
    @Published var isProcessing = false
    
    // å›è°ƒ
    var onTranscriptionUpdate: ((String) -> Void)?
    var onSummaryGenerated: ((String, AudioType) -> Void)?
    var onAudioTypeDetected: ((AudioType) -> Void)?
    
    override init() {
        super.init()
        urlSession = URLSession(configuration: .default, delegate: self, delegateQueue: OperationQueue())
        inputNode = audioEngine.inputNode
    }
    
    // MARK: - WebSocketè¿æ¥ç®¡ç†
    
    func connect() {
        guard let url = URL(string: wsURL) else { return }
        
        var request = URLRequest(url: url)
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        
        webSocketTask = urlSession.webSocketTask(with: request)
        webSocketTask?.resume()
        
        // å¼€å§‹æ¥æ”¶æ¶ˆæ¯
        receiveMessage()
        
        // å‘é€sessioné…ç½®
        Task {
            await configureSession()
        }
    }
    
    func disconnect() {
        webSocketTask?.cancel(with: .goingAway, reason: nil)
        webSocketTask = nil
        isConnected = false
        stopRecording()
    }
    
    // MARK: - Sessioné…ç½®
    
    private func configureSession() async {
        let sessionConfig: [String: Any] = [
            "event_id": UUID().uuidString,
            "type": "session.update",
            "session": [
                "modalities": ["text", "audio"],
                "instructions": """
                ä½ æ˜¯ä¸“ä¸šçš„å®æ—¶éŸ³é¢‘åˆ†æåŠ©æ‰‹ã€‚ä»»åŠ¡ï¼š
                
                **éŸ³é¢‘åˆ†ç±»**ï¼š
                1. äººå£°å†…å®¹ - å¯¹è¯ã€ç‹¬ç™½ã€æ­Œå£°ç­‰
                2. ç¯å¢ƒéŸ³ - éŸ³ä¹ã€è‡ªç„¶å£°ã€æœºæ¢°å£°ã€å™ªéŸ³ç­‰
                
                **å¤„ç†è§„åˆ™**ï¼š
                - æ£€æµ‹åˆ°äººå£°ï¼šè½¬å½•å†…å®¹ + 5-10å­—æ€»ç»“
                - æ£€æµ‹åˆ°ç¯å¢ƒéŸ³ï¼šè¯†åˆ«å£°éŸ³ç±»å‹ + 5å­—æè¿°
                - æ··åˆå£°éŸ³ï¼šä¼˜å…ˆè¯†åˆ«ä¸»è¦å£°æº
                
                **è¾“å‡ºæ ¼å¼**ï¼š
                äººå£°: "æ€»ç»“: [å†…å®¹]"
                ç¯å¢ƒéŸ³: "å£°éŸ³: [ç±»å‹]"
                
                **å“åº”è¦æ±‚**ï¼š
                - 5-10ç§’å†…å®Œæˆåˆ†æ
                - ä¿æŒç®€æ´å‡†ç¡®
                - ä¸­æ–‡å›å¤
                
                è¯·ä½¿ç”¨é»˜è®¤å¥³å£°ä¸ç”¨æˆ·äº¤æµ
                """,
                "voice": "qingchunshaonv",
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "turn_detection": [
                    "type": "server_vad",
                    "threshold": 0.3,  // é™ä½é˜ˆå€¼ï¼Œå¯¹ç¯å¢ƒéŸ³æ›´æ•æ„Ÿ
                    "prefix_padding_ms": 200,
                    "silence_duration_ms": 1000  // å¢åŠ é™é»˜æ—¶é—´ï¼Œç»™ç¯å¢ƒéŸ³æ›´å¤šæ£€æµ‹æ—¶é—´
                ]
            ]
        ]
        
        await sendMessage(sessionConfig)
    }
    
    // MARK: - æ¶ˆæ¯å‘é€ä¸æ¥æ”¶
    
    private func sendMessage(_ message: [String: Any]) async {
        guard let webSocketTask = webSocketTask,
              let jsonData = try? JSONSerialization.data(withJSONObject: message),
              let jsonString = String(data: jsonData, encoding: .utf8) else { return }
        
        let message = URLSessionWebSocketTask.Message.string(jsonString)
        
        do {
            try await webSocketTask.send(message)
        } catch {
            print("âŒ å‘é€æ¶ˆæ¯å¤±è´¥: \(error)")
        }
    }
    
    private func receiveMessage() {
        webSocketTask?.receive { [weak self] result in
            switch result {
            case .success(let message):
                switch message {
                case .string(let text):
                    self?.handleServerEvent(text)
                case .data(let data):
                    if let text = String(data: data, encoding: .utf8) {
                        self?.handleServerEvent(text)
                    }
                @unknown default:
                    break
                }
                
                // ç»§ç»­æ¥æ”¶ä¸‹ä¸€æ¡æ¶ˆæ¯
                self?.receiveMessage()
                
            case .failure(let error):
                print("âŒ æ¥æ”¶æ¶ˆæ¯å¤±è´¥: \(error)")
                self?.isConnected = false
            }
        }
    }
    
    private func handleServerEvent(_ jsonString: String) {
        guard let data = jsonString.data(using: .utf8),
              let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
              let type = json["type"] as? String else { return }
        
        DispatchQueue.main.async { [weak self] in
            switch type {
            case "session.created", "session.updated":
                self?.isConnected = true
                print("âœ… Sessionå·²å»ºç«‹")
                
            case "input_audio_buffer.speech_started":
                // VADæ£€æµ‹åˆ°è¯´è¯å¼€å§‹
                print("ğŸ¤ æ£€æµ‹åˆ°éŸ³é¢‘è¾“å…¥å¼€å§‹")
                self?.isProcessing = true
                self?.processingStartTime = Date()
                
            case "conversation.item.input_audio_transcription.completed":
                // ç”¨æˆ·éŸ³é¢‘è½¬å½•å®Œæˆ
                if let transcript = json["transcript"] as? String {
                    self?.currentTranscription = transcript
                    self?.onTranscriptionUpdate?(transcript)
                    
                    // è§¦å‘æ™ºèƒ½åˆ†æå’Œæ€»ç»“
                    self?.requestSmartAnalysis(for: transcript)
                }
                
            case "response.audio_transcript.delta":
                // AIå›å¤çš„æ–‡å­—æµï¼ˆåˆ†æç»“æœï¼‰
                if let delta = json["delta"] as? String {
                    self?.currentSummary += delta
                }
                
            case "response.audio_transcript.done":
                // AIå›å¤å®Œæˆ
                if let transcript = json["transcript"] as? String {
                    let (summary, detectedType) = self?.parseAIResponse(transcript) ?? ("", .unknown)
                    
                    self?.currentSummary = summary
                    self?.audioType = detectedType
                    self?.isProcessing = false
                    
                    // è®¡ç®—å¤„ç†æ—¶é—´
                    let processingTime = Date().timeIntervalSince(self?.processingStartTime ?? Date())
                    print("â±ï¸ å¤„ç†è€—æ—¶: \(String(format: "%.1f", processingTime))ç§’")
                    
                    self?.onSummaryGenerated?(summary, detectedType)
                    self?.onAudioTypeDetected?(detectedType)
                    
                    // ä¿å­˜åˆ°å†å²
                    let item = ConversationItem(
                        transcription: self?.currentTranscription ?? "",
                        summary: summary,
                        timestamp: Date(),
                        audioType: detectedType.rawValue
                    )
                    self?.conversationHistory.append(item)
                }
                
            case "input_audio_buffer.speech_started":
                // VADæ£€æµ‹åˆ°è¯´è¯å¼€å§‹
                print("ğŸ¤ æ£€æµ‹åˆ°è¯´è¯å¼€å§‹")
                
            case "input_audio_buffer.speech_stopped":
                // VADæ£€æµ‹åˆ°è¯´è¯ç»“æŸ
                print("ğŸ”‡ æ£€æµ‹åˆ°è¯´è¯ç»“æŸ")
                
            case "error":
                if let error = json["error"] as? [String: Any],
                   let message = error["message"] as? String {
                    print("âŒ æœåŠ¡å™¨é”™è¯¯: \(message)")
                }
                
            default:
                break
            }
        }
    }
    
    // MARK: - éŸ³é¢‘å½•åˆ¶ä¸å‘é€
    
    func startRecording() {
        guard !isRecording else { return }
        
        setupAudioSession()
        
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, _ in
            self?.processAudioBuffer(buffer)
        }
        
        do {
            try audioEngine.start()
            isRecording = true
            print("ğŸ¤ å¼€å§‹å½•éŸ³")
        } catch {
            print("âŒ å¯åŠ¨å½•éŸ³å¤±è´¥: \(error)")
        }
    }
    
    func stopRecording() {
        guard isRecording else { return }
        
        audioEngine.stop()
        inputNode.removeTap(onBus: 0)
        isRecording = false
        
        // æäº¤éŸ³é¢‘ç¼“å†²åŒº
        Task {
            await commitAudioBuffer()
        }
        
        print("ğŸ›‘ åœæ­¢å½•éŸ³")
    }
    
    private func setupAudioSession() {
        let session = AVAudioSession.sharedInstance()
        
        do {
            try session.setCategory(.playAndRecord, mode: .default)
            try session.setActive(true)
        } catch {
            print("âŒ éŸ³é¢‘ä¼šè¯è®¾ç½®å¤±è´¥: \(error)")
        }
    }
    
    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer) {
        // è½¬æ¢ä¸ºPCM16æ ¼å¼
        guard let pcm16Data = convertToPCM16(buffer: buffer) else { return }
        
        // Base64ç¼–ç 
        let base64Audio = pcm16Data.base64EncodedString()
        
        // å‘é€åˆ°æœåŠ¡å™¨
        Task {
            let message: [String: Any] = [
                "event_id": UUID().uuidString,
                "type": "input_audio_buffer.append",
                "audio": base64Audio
            ]
            await sendMessage(message)
        }
    }
    
    private func convertToPCM16(buffer: AVAudioPCMBuffer) -> Data? {
        guard let channelData = buffer.int16ChannelData else {
            // å¦‚æœä¸æ˜¯int16æ ¼å¼ï¼Œéœ€è¦è½¬æ¢
            return convertFloatToPCM16(buffer: buffer)
        }
        
        let channelCount = Int(buffer.format.channelCount)
        let frameLength = Int(buffer.frameLength)
        
        var data = Data()
        for frame in 0..<frameLength {
            for channel in 0..<channelCount {
                let sample = channelData[channel][frame]
                data.append(contentsOf: withUnsafeBytes(of: sample) { Array($0) })
            }
        }
        
        return data
    }
    
    private func convertFloatToPCM16(buffer: AVAudioPCMBuffer) -> Data? {
        guard let channelData = buffer.floatChannelData else { return nil }
        
        let channelCount = Int(buffer.format.channelCount)
        let frameLength = Int(buffer.frameLength)
        
        var data = Data()
        for frame in 0..<frameLength {
            for channel in 0..<channelCount {
                let floatSample = channelData[channel][frame]
                let int16Sample = Int16(max(-32768, min(32767, floatSample * 32768)))
                data.append(contentsOf: withUnsafeBytes(of: int16Sample) { Array($0) })
            }
        }
        
        return data
    }
    
    private func commitAudioBuffer() async {
        let message: [String: Any] = [
            "event_id": UUID().uuidString,
            "type": "input_audio_buffer.commit"
        ]
        await sendMessage(message)
    }
    
    // MARK: - æ™ºèƒ½åˆ†æåŠŸèƒ½
    
    private func requestSmartAnalysis(for text: String) {
        Task {
            // å¦‚æœæ˜¯ç¯å¢ƒéŸ³æˆ–æ— è½¬å½•å†…å®¹ï¼Œç›´æ¥é€šè¿‡VADå¤„ç†
            if text.isEmpty || text.count < 3 {
                await requestEnvironmentAnalysis()
                return
            }
            
            // æœ‰å†…å®¹åˆ™è¿›è¡Œæ™ºèƒ½åˆ†æ
            let message: [String: Any] = [
                "event_id": UUID().uuidString,
                "type": "conversation.item.create", 
                "item": [
                    "id": UUID().uuidString,
                    "type": "message",
                    "role": "user",
                    "content": [
                        [
                            "type": "input_text",
                            "text": "åˆ†æè¿™æ®µéŸ³é¢‘å†…å®¹ï¼š\(text)"
                        ]
                    ]
                ]
            ]
            await sendMessage(message)
            
            // è§¦å‘æ¨ç†
            let responseMessage: [String: Any] = [
                "event_id": UUID().uuidString,
                "type": "response.create"
            ]
            await sendMessage(responseMessage)
        }
    }
    
    private func requestEnvironmentAnalysis() async {
        let message: [String: Any] = [
            "event_id": UUID().uuidString,
            "type": "conversation.item.create",
            "item": [
                "id": UUID().uuidString,
                "type": "message", 
                "role": "user",
                "content": [
                    [
                        "type": "input_text",
                        "text": "åˆšæ‰æ£€æµ‹åˆ°ç¯å¢ƒéŸ³é¢‘ï¼Œè¯·è¯†åˆ«å£°éŸ³ç±»å‹"
                    ]
                ]
            ]
        ]
        await sendMessage(message)
        
        let responseMessage: [String: Any] = [
            "event_id": UUID().uuidString,
            "type": "response.create"
        ]
        await sendMessage(responseMessage)
    }
    
    private func parseAIResponse(_ response: String) -> (String, AudioType) {
        print("ğŸ¤– AIåŸå§‹å›å¤: \(response)")
        
        // è§£æå›å¤æ ¼å¼
        if response.contains("æ€»ç»“:") {
            // äººå£°å†…å®¹
            let summary = response.replacingOccurrences(of: "æ€»ç»“:", with: "").trimmingCharacters(in: .whitespacesAndNewlines)
            
            // æ ¹æ®å†…å®¹æ¨æ–­å…·ä½“ç±»å‹
            let audioType = inferAudioType(from: response)
            return (summary, audioType)
            
        } else if response.contains("å£°éŸ³:") {
            // ç¯å¢ƒéŸ³
            let soundType = response.replacingOccurrences(of: "å£°éŸ³:", with: "").trimmingCharacters(in: .whitespacesAndNewlines)
            let audioType = classifyEnvironmentSound(soundType)
            return (soundType, audioType)
            
        } else {
            // é€šç”¨å¤„ç†ï¼šç›´æ¥æˆªå–å‰10ä¸ªå­—ä½œä¸ºæ€»ç»“
            let summary = String(response.prefix(10))
            let audioType = inferAudioType(from: response)
            return (summary, audioType)
        }
    }
    
    private func inferAudioType(from content: String) -> AudioType {
        let lowercased = content.lowercased()
        
        if lowercased.contains("å¯¹è¯") || lowercased.contains("äº¤è°ˆ") {
            return .conversation
        } else if lowercased.contains("æ­Œ") || lowercased.contains("å”±") {
            return .singing
        } else if lowercased.contains("éŸ³ä¹") {
            return .music
        } else if lowercased.contains("æœºå™¨") || lowercased.contains("è®¾å¤‡") {
            return .mechanical
        } else if lowercased.contains("è‡ªç„¶") || lowercased.contains("é£") || lowercased.contains("é›¨") {
            return .nature
        } else if lowercased.contains("å™ªéŸ³") || lowercased.contains("åµ") {
            return .noise
        } else {
            return .humanVoice
        }
    }
    
    private func classifyEnvironmentSound(_ soundDescription: String) -> AudioType {
        let lowercased = soundDescription.lowercased()
        
        if lowercased.contains("éŸ³ä¹") || lowercased.contains("æ­Œ") {
            return .music
        } else if lowercased.contains("è‡ªç„¶") || lowercased.contains("é£") || lowercased.contains("é›¨") || lowercased.contains("é¸Ÿ") {
            return .nature
        } else if lowercased.contains("æœº") || lowercased.contains("ç”µ") || lowercased.contains("è½¦") {
            return .mechanical
        } else if lowercased.contains("å™ª") || lowercased.contains("æ‚") {
            return .noise
        } else {
            return .unknown
        }
    }
}

// MARK: - URLSessionWebSocketDelegate

extension StepRealtimeManager: URLSessionWebSocketDelegate {
    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didOpenWithProtocol protocol: String?) {
        print("âœ… WebSocketè¿æ¥å·²å»ºç«‹")
    }
    
    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didCloseWith closeCode: URLSessionWebSocketTask.CloseCode, reason: Data?) {
        print("ğŸ”Œ WebSocketè¿æ¥å·²å…³é—­")
        isConnected = false
    }
}

// MARK: - æ•°æ®æ¨¡å‹

struct ConversationItem: Identifiable {
    let id = UUID()
    let transcription: String
    let summary: String
    let timestamp: Date
    let audioType: String = "unknown"
}