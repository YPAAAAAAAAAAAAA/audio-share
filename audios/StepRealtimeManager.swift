import Foundation
import AVFoundation
import UIKit

// éŸ³é¢‘ç±»å‹æšä¸¾
enum AudioType: String, CaseIterable {
    case humanVoice = "human_voice"      // äººå£°
    case music = "music"                 // éŸ³ä¹
    case nature = "nature"               // è‡ªç„¶ç¯å¢ƒéŸ³
    case mechanical = "mechanical"        // æœºæ¢°å£°
    case conversation = "conversation"    // å¯¹è¯
    case singing = "singing"             // æ­Œå£°
    case noise = "noise"                 // å™ªéŸ³
    case unknown = "unknown"             // æœªçŸ¥
    
    var displayName: String {
        switch self {
        case .humanVoice: return "äººå£°"
        case .music: return "éŸ³ä¹"
        case .nature: return "è‡ªç„¶éŸ³"
        case .mechanical: return "æœºæ¢°éŸ³"
        case .conversation: return "å¯¹è¯"
        case .singing: return "æ­Œå£°"
        case .noise: return "å™ªéŸ³"
        case .unknown: return "æœªçŸ¥"
        }
    }
    
    var icon: String {
        switch self {
        case .humanVoice: return "person.wave.2"
        case .music: return "music.note"
        case .nature: return "leaf"
        case .mechanical: return "gearshape"
        case .conversation: return "bubble.left.and.bubble.right"
        case .singing: return "music.mic"
        case .noise: return "waveform"
        case .unknown: return "questionmark"
        }
    }
    
    var color: UIColor {
        switch self {
        case .humanVoice: return .systemBlue
        case .music: return .systemPurple
        case .nature: return .systemGreen
        case .mechanical: return .systemOrange
        case .conversation: return .systemTeal
        case .singing: return .systemPink
        case .noise: return .systemRed
        case .unknown: return .systemGray
        }
    }
}

// Step Realtime API WebSocketå®¢æˆ·ç«¯
class StepRealtimeManager: NSObject, ObservableObject {
    
    // APIé…ç½®
    private let apiKey = "8FyDGELcpTdfh1JNOoePkfXzCtExQHL8DSdEX9UYfl4dCsE77R4WIUOIJqanw0Cl"
    private let wsURL = "wss://api.stepfun.com/v1/realtime"
    
    // WebSocketè¿æ¥
    private var webSocketTask: URLSessionWebSocketTask?
    private var urlSession: URLSession!
    
    // è¿æ¥ç®¡ç†å’Œé‡è¿
    private var reconnectTimer: Timer?
    private var reconnectAttempts = 0
    private var maxReconnectAttempts = 5
    private var reconnectInterval: TimeInterval = 2.0
    private var pingTimer: Timer?
    private var connectionHealthTimer: Timer?
    private var isManualDisconnect = false
    
    // è¿æ¥ç¨³å®šæ€§ç®¡ç†
    private var lastConnectionFailure: Date?
    private var consecutiveFailures = 0
    private var circuitBreakerOpen = false
    private var circuitBreakerTimeout: TimeInterval = 60.0
    
    // éŸ³é¢‘å¤„ç†
    private var audioEngine = AVAudioEngine()
    private var inputNode: AVAudioInputNode!
    
    // çŠ¶æ€ç®¡ç†
    @Published var isConnected = false
    @Published var isRecording = false
    @Published var currentTranscription = ""
    @Published var currentSummary = ""
    @Published var audioType: AudioType = .unknown
    @Published var conversationHistory: [ConversationItem] = []
    @Published var connectionStatus: ConnectionStatus = .disconnected
    
    // è¿æ¥çŠ¶æ€æšä¸¾
    enum ConnectionStatus: String, CaseIterable {
        case disconnected = "å·²æ–­å¼€"
        case connecting = "è¿æ¥ä¸­"
        case connected = "å·²è¿æ¥"
        case reconnecting = "é‡è¿ä¸­"
        case failed = "è¿æ¥å¤±è´¥"
    }
    
    // å®æ—¶å¤„ç†çŠ¶æ€
    @Published var processingStartTime: Date?
    @Published var isProcessing = false
    
    // å›è°ƒ
    var onTranscriptionUpdate: ((String) -> Void)?
    var onSummaryGenerated: ((String, AudioType) -> Void)?
    var onAudioTypeDetected: ((AudioType) -> Void)?
    
    override init() {
        super.init()
        urlSession = URLSession(configuration: .default, delegate: self, delegateQueue: OperationQueue())
        inputNode = audioEngine.inputNode
    }
    
    // MARK: - WebSocketè¿æ¥ç®¡ç†
    
    // Real content analysis using WebSocket for immediate audio processing
    func analyzeAudioWithHTTP(audioURL: String, duration: Int) async throws -> (summary: String, transcription: String, audioType: AudioType) {
        print("ğŸ¯ ä½¿ç”¨WebSocketå®æ—¶åˆ†æéŸ³é¢‘å†…å®¹...")
        print("ğŸ”— éŸ³é¢‘URL: \(audioURL)")
        print("â±ï¸ éŸ³é¢‘æ—¶é•¿: \(duration)ç§’")
        
        return try await withCheckedThrowingContinuation { continuation in
            // Start analysis with actual audio content
            Task {
                do {
                    let result = try await processAudioViaWebSocket(audioURL: audioURL, duration: duration)
                    print("âœ… WebSocketåˆ†ææˆåŠŸ: \(result.summary)")
                    continuation.resume(returning: result)
                } catch {
                    // Fallback to smart summary if WebSocket fails
                    print("âŒ WebSocketåˆ†æå¤±è´¥ï¼Œä½¿ç”¨æ™ºèƒ½æ‘˜è¦: \(error)")
                    print("ğŸ” å¤±è´¥è¯¦æƒ…: \(error.localizedDescription)")
                    let fallback = await generateFallbackSummary(duration: duration)
                    print("ğŸ”„ ä½¿ç”¨fallbackæ‘˜è¦: \(fallback.summary)")
                    continuation.resume(returning: fallback)
                }
            }
        }
    }
    
    // Process audio through WebSocket for real content analysis
    private func processAudioViaWebSocket(audioURL: String, duration: Int) async throws -> (summary: String, transcription: String, audioType: AudioType) {
        // Download audio data and convert to PCM16 if needed
        guard let url = URL(string: audioURL) else {
            throw NSError(domain: "StepRealtimeManager", code: 1, userInfo: [NSLocalizedDescriptionKey: "Invalid audio URL"])
        }
        
        let (audioData, _) = try await URLSession.shared.data(from: url)
        let pcmData = try convertAudioToPCM16(audioData: audioData)
        
        // Connect to WebSocket and process audio
        return try await processAudioData(pcmData: pcmData, duration: duration)
    }
    
    // Convert audio data to PCM16 format
    private func convertAudioToPCM16(audioData: Data) throws -> Data {
        // Check if this is a WAV file by looking for WAV header
        if audioData.count > 44 && audioData.starts(with: Data("RIFF".utf8)) {
            // Skip WAV header (44 bytes) and return raw PCM data
            let headerSize = 44
            if audioData.count > headerSize {
                return audioData.dropFirst(headerSize)
            }
        }
        
        // If it's not a WAV file or too small, return as-is
        // (assume it's already raw PCM16 data)
        return audioData
    }
    
    // Process PCM16 audio data through WebSocket
    private func processAudioData(pcmData: Data, duration: Int) async throws -> (summary: String, transcription: String, audioType: AudioType) {
        // Clear previous analysis transcription
        currentAnalysisTranscription = ""
        // Try different connection approaches to handle -1011 errors
        let connectionMethods = [
            ("Standard", { self.createAnalysisWebSocket() }),
            ("Minimal Headers", { self.createMinimalAnalysisWebSocket() }),
            ("HTTP Upgrade", { self.createHTTPUpgradeWebSocket() })
        ]
        
        var analysisConnection: URLSessionWebSocketTask?
        
        for (methodName, createConnection) in connectionMethods {
            print("ğŸ”„ Trying connection method: \(methodName)")
            
            analysisConnection = createConnection()
            
            // Wait for connection to establish
            try? await Task.sleep(nanoseconds: 3_000_000_000) // 3 seconds
            
            if analysisConnection?.state == .running {
                print("âœ… åˆ†æè¿æ¥æˆåŠŸï¼Œä½¿ç”¨æ–¹æ³•: \(methodName)")
                break
            } else {
                print("âŒ æ–¹æ³• \(methodName) è¿æ¥å¤±è´¥ï¼ŒçŠ¶æ€: \(analysisConnection?.state.rawValue ?? -1)")
                analysisConnection?.cancel()
                analysisConnection = nil
                
                // Wait before trying next method
                try? await Task.sleep(nanoseconds: 1_000_000_000) // 1 second
            }
        }
        
        // If all methods failed, use fallback
        guard let validConnection = analysisConnection, validConnection.state == .running else {
            print("âŒ æ‰€æœ‰è¿æ¥æ–¹æ³•å‡å¤±è´¥ï¼Œä½¿ç”¨fallback")
            let fallback = await generateFallbackSummary(duration: duration)
            return fallback
        }
        
        return try await withCheckedThrowingContinuation { continuation in
            var hasReturned = false
            
            // Set up analysis completion handler
            self.analysisCompletionHandler = { summary, transcription, audioType in
                if !hasReturned {
                    hasReturned = true
                    continuation.resume(returning: (summary, transcription, audioType))
                }
            }
            
            // Send audio data for analysis
            Task {
                await sendAudioForAnalysis(pcmData: pcmData, connection: validConnection)
            }
            
            // Timeout after 30 seconds
            Task {
                try await Task.sleep(nanoseconds: 30_000_000_000)
                if !hasReturned {
                    hasReturned = true
                    print("â° WebSocketåˆ†æè¶…æ—¶ï¼Œä½¿ç”¨fallbackæ‘˜è¦")
                    let fallback = await generateFallbackSummary(duration: duration)
                    print("ğŸ”„ è¶…æ—¶fallbackæ‘˜è¦: \(fallback.summary)")
                    continuation.resume(returning: fallback)
                }
            }
        }
    }
    
    // Analysis completion handler
    private var analysisCompletionHandler: ((String, String, AudioType) -> Void)?
    
    // Store WebSocket transcription for current analysis
    private var currentAnalysisTranscription: String = ""
    
    // Send audio data for analysis
    private func sendAudioForAnalysis(pcmData: Data, connection: URLSessionWebSocketTask) async {
        // Send session configuration for analysis
        let sessionConfig: [String: Any] = [
            "event_id": UUID().uuidString,
            "type": "session.update",
            "session": [
                "model": "step-audio-2-mini",
                "modalities": ["text", "audio"],
                "instructions": """
                åˆ†æéŸ³é¢‘ï¼Œåªè¾“å‡ºï¼š
                emoji:summary

                è§„åˆ™ï¼š
                1. emojiåªèƒ½1ä¸ª
                2. summaryé™5-8ä¸ªå­—
                3. ä¸è¦åŠ ä»»ä½•æ ‡è®°æˆ–ç¬¦å·
                4. å”±æ­Œä¼˜å…ˆè¯†åˆ«ä¸ºğŸµ
                5. ä¸è¦è¾“å‡º<|EOT|>æˆ–å…¶ä»–ç»“æŸæ ‡è®°
                
                ç¤ºä¾‹ï¼š
                ğŸµ:å†›æ­Œå˜¹äº®
                ğŸ˜Š:å¼€å¿ƒèŠå¤©
                ğŸŒ§ï¸:é›¨å£°ç¯å¢ƒ
                """,
                "voice": "qingchunshaonv",
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "turn_detection": NSNull()
            ]
        ]
        
        await sendAnalysisMessage(sessionConfig, to: connection)
        
        // Send audio data in chunks
        let chunkSize = 4096
        let base64Audio = pcmData.base64EncodedString()
        
        for i in stride(from: 0, to: base64Audio.count, by: chunkSize) {
            let endIndex = min(i + chunkSize, base64Audio.count)
            let chunk = String(base64Audio[base64Audio.index(base64Audio.startIndex, offsetBy: i)..<base64Audio.index(base64Audio.startIndex, offsetBy: endIndex)])
            
            let audioMessage: [String: Any] = [
                "event_id": UUID().uuidString,
                "type": "input_audio_buffer.append",
                "audio": chunk
            ]
            
            await sendAnalysisMessage(audioMessage, to: connection)
        }
        
        // Commit audio buffer for processing
        let commitMessage: [String: Any] = [
            "event_id": UUID().uuidString,
            "type": "input_audio_buffer.commit"
        ]
        
        await sendAnalysisMessage(commitMessage, to: connection)
        
        // Request response
        let responseMessage: [String: Any] = [
            "event_id": UUID().uuidString,
            "type": "response.create"
        ]
        
        await sendAnalysisMessage(responseMessage, to: connection)
    }
    
    // Create analysis WebSocket connection with improved stability
    private func createAnalysisWebSocket() -> URLSessionWebSocketTask {
        // Model parameter is REQUIRED in URL for Step API
        let urlWithModel = "\(wsURL)?model=step-audio-2-mini"
        guard let url = URL(string: urlWithModel) else {
            fatalError("Invalid WebSocket URL")
        }
        
        var request = URLRequest(url: url)
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        request.timeoutInterval = 30.0  // Shorter timeout for faster failure detection
        
        // Use minimal headers to avoid handshake conflicts
        request.setValue("websocket", forHTTPHeaderField: "Upgrade")
        request.setValue("Upgrade", forHTTPHeaderField: "Connection")
        request.setValue("13", forHTTPHeaderField: "Sec-WebSocket-Version")
        
        // Add required Step API headers
        request.setValue("step-audio-2-mini", forHTTPHeaderField: "X-Model")
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        
        print("ğŸ”— Connecting to WebSocket: \(urlWithModel)")
        print("ğŸ”‘ Authorization: Bearer \(String(apiKey.prefix(10)))...")
        print("ğŸ“‹ Model: step-audio-2-mini")
        
        let task = urlSession.webSocketTask(with: request)
        
        task.resume()
        
        // Monitor connection state
        Task {
            for attempt in 1...10 { // Check for 5 seconds
                try? await Task.sleep(nanoseconds: 500_000_000) // 0.5 seconds each
                
                switch task.state {
                case .running:
                    print("âœ… Analysis WebSocket connected successfully")
                    // Start ping timer only after successful connection
                    self.startPingTimer(for: task)
                    return
                case .completed, .canceling:
                    print("âŒ Analysis WebSocket connection failed: state \(task.state.rawValue)")
                    return
                default:
                    if attempt == 10 {
                        print("â° Analysis WebSocket connection timeout")
                    }
                }
            }
        }
        
        // Set up message receiving
        receiveAnalysisMessageWithRetry(from: task)
        
        return task
    }
    
    // Alternative connection methods for handling -1011 errors
    private func createMinimalAnalysisWebSocket() -> URLSessionWebSocketTask {
        let urlWithModel = "\(wsURL)?model=step-audio-2-mini"
        guard let url = URL(string: urlWithModel) else {
            fatalError("Invalid WebSocket URL")
        }
        
        var request = URLRequest(url: url)
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        request.timeoutInterval = 30.0
        
        // Only essential WebSocket headers
        request.setValue("websocket", forHTTPHeaderField: "Upgrade")
        request.setValue("Upgrade", forHTTPHeaderField: "Connection")
        
        print("ğŸ”— Minimal WebSocket connection to: \(urlWithModel)")
        
        let task = urlSession.webSocketTask(with: request)
        task.resume()
        receiveAnalysisMessageWithRetry(from: task)
        
        return task
    }
    
    private func createHTTPUpgradeWebSocket() -> URLSessionWebSocketTask {
        // Try with query parameters in URL
        let urlWithParams = "\(wsURL)?model=step-audio-2-mini&format=json"
        guard let url = URL(string: urlWithParams) else {
            fatalError("Invalid WebSocket URL")
        }
        
        var request = URLRequest(url: url)
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        request.timeoutInterval = 30.0
        
        // Standard HTTP to WebSocket upgrade headers
        request.setValue("websocket", forHTTPHeaderField: "Upgrade")
        request.setValue("Upgrade", forHTTPHeaderField: "Connection")
        request.setValue("13", forHTTPHeaderField: "Sec-WebSocket-Version")
        request.setValue("*", forHTTPHeaderField: "Sec-WebSocket-Extensions")
        
        print("ğŸ”— HTTP Upgrade WebSocket connection to: \(urlWithParams)")
        
        let task = urlSession.webSocketTask(with: request)
        task.resume()
        receiveAnalysisMessageWithRetry(from: task)
        
        return task
    }
    
    // Keep connection alive with periodic pings
    private func startPingTimer(for task: URLSessionWebSocketTask) {
        pingTimer?.invalidate()
        pingTimer = Timer.scheduledTimer(withTimeInterval: 20.0, repeats: true) { [weak self, weak task] _ in
            guard let task = task, task.state == .running else { 
                self?.handleConnectionFailure()
                return 
            }
            
            task.sendPing { [weak self] error in
                if let error = error {
                    print("âŒ Ping failed: \(error)")
                    self?.handleConnectionFailure()
                } else {
                    print("ğŸ“ Ping successful")
                }
            }
        }
    }
    
    // Send message to analysis WebSocket with retry logic
    private func sendAnalysisMessage(_ message: [String: Any], to connection: URLSessionWebSocketTask, retryCount: Int = 0) async {
        // Check connection state first
        guard connection.state == .running else {
            print("âŒ åˆ†æWebSocketè¿æ¥çŠ¶æ€å¼‚å¸¸: \(connection.state.rawValue)")
            if retryCount < 2 {
                // Wait and retry
                try? await Task.sleep(nanoseconds: 1_000_000_000)
                await sendAnalysisMessage(message, to: connection, retryCount: retryCount + 1)
            }
            return
        }
        
        guard let jsonData = try? JSONSerialization.data(withJSONObject: message),
              let jsonString = String(data: jsonData, encoding: .utf8) else {
            print("âŒ åˆ†ææ¶ˆæ¯åºåˆ—åŒ–å¤±è´¥")
            return
        }
        
        let wsMessage = URLSessionWebSocketTask.Message.string(jsonString)
        
        do {
            try await connection.send(wsMessage)
            
            // Log success for non-audio messages
            if let type = message["type"] as? String, type != "input_audio_buffer.append" {
                print("âœ… åˆ†ææ¶ˆæ¯å‘é€æˆåŠŸ: \(type)")
            }
        } catch {
            print("âŒ å‘é€åˆ†ææ¶ˆæ¯å¤±è´¥: \(error)")
            
            // Handle specific error types
            if let urlError = error as? URLError {
                switch urlError.code {
                case .networkConnectionLost, .timedOut:
                    print("ğŸŒ ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œå°†é‡è¯•")
                    if retryCount < 2 {
                        try? await Task.sleep(nanoseconds: 2_000_000_000) // 2 seconds
                        await sendAnalysisMessage(message, to: connection, retryCount: retryCount + 1)
                    }
                default:
                    print("ğŸ” URLError details: \(urlError.localizedDescription)")
                }
            }
        }
    }
    
    // Receive messages from analysis WebSocket with retry
    private func receiveAnalysisMessageWithRetry(from connection: URLSessionWebSocketTask) {
        connection.receive { [weak self] result in
            switch result {
            case .success(let message):
                switch message {
                case .string(let text):
                    self?.handleAnalysisEvent(text)
                case .data(let data):
                    if let text = String(data: data, encoding: .utf8) {
                        self?.handleAnalysisEvent(text)
                    }
                @unknown default:
                    break
                }
                
                // Continue receiving
                self?.receiveAnalysisMessageWithRetry(from: connection)
                
            case .failure(let error):
                print("âŒ åˆ†ææ¶ˆæ¯æ¥æ”¶å¤±è´¥: \(error)")
                print("ğŸ” è¿æ¥çŠ¶æ€: \(connection.state.rawValue)")
                
                // Don't retry if connection is cancelled
                if connection.state != .canceling && connection.state != .completed {
                    // Retry after a short delay
                    DispatchQueue.global().asyncAfter(deadline: .now() + 1.0) { [weak self] in
                        print("ğŸ”„ é‡è¯•æ¥æ”¶æ¶ˆæ¯...")
                        self?.receiveAnalysisMessageWithRetry(from: connection)
                    }
                }
            }
        }
    }
    
    // Legacy function for backward compatibility
    private func receiveAnalysisMessage(from connection: URLSessionWebSocketTask) {
        receiveAnalysisMessageWithRetry(from: connection)
    }
    
    // Handle analysis WebSocket events
    private func handleAnalysisEvent(_ jsonString: String) {
        print("ğŸ“¥ åˆ†æäº‹ä»¶: \(jsonString)")
        
        guard let data = jsonString.data(using: .utf8),
              let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
              let type = json["type"] as? String else {
            return
        }
        
        switch type {
        case "conversation.item.input_audio_transcription.completed":
            // Got transcription
            if let transcript = json["transcript"] as? String {
                DispatchQueue.main.async {
                    self.currentTranscription = transcript
                    // Store for current analysis
                    self.currentAnalysisTranscription = transcript
                    // ğŸ”§ CRITICAL FIX: Also trigger callback in HTTP analysis path
                    self.onTranscriptionUpdate?(transcript)
                }
                print("ğŸ“ è½¬å½•å®Œæˆ: \(transcript)")
                print("ğŸ”§ å­˜å‚¨åˆ†æè½¬å½•: \(transcript)")
                print("ğŸ”§ è§¦å‘è½¬å½•å›è°ƒ: onTranscriptionUpdate")
            }
            
        case "response.audio_transcript.done":
            // Got AI analysis response - WAIT for transcription before completing
            if let transcript = json["transcript"] as? String {
                // Clean EOT marker immediately
                let cleanTranscript = transcript
                    .replacingOccurrences(of: "<|EOT|>", with: "")
                    .replacingOccurrences(of: "<|eot|>", with: "")
                    .trimmingCharacters(in: .whitespacesAndNewlines)
                
                print("ğŸ¤– AIå®Œæ•´å“åº”: \(cleanTranscript)")
                let (summary, audioType) = parseAIResponse(cleanTranscript)
                
                print("âœ… AIåˆ†æå®Œæˆ: æ€»ç»“=\(summary), ç±»å‹=\(audioType.displayName)")
                print("ğŸ” å½“å‰åˆ†æè½¬å½•: '\(currentAnalysisTranscription)'")
                
                // ğŸ”§ CRITICAL FIX: Wait for transcription if not yet received  
                if currentAnalysisTranscription.isEmpty {
                    print("â° AIå®Œæˆä½†è½¬å½•æœªæ”¶åˆ°ï¼Œå»¶è¿Ÿ3ç§’ç­‰å¾…è½¬å½•...")
                    DispatchQueue.global().asyncAfter(deadline: .now() + 3.0) {
                        let finalTranscription = self.currentAnalysisTranscription.isEmpty ? cleanTranscript : self.currentAnalysisTranscription
                        print("ğŸ”§ å»¶è¿Ÿå®Œæˆï¼Œä½¿ç”¨è½¬å½•: '\(finalTranscription)'")
                        self.analysisCompletionHandler?(summary, finalTranscription, audioType)
                    }
                } else {
                    // Transcription already received, complete immediately
                    print("ğŸ”§ ç«‹å³å®Œæˆï¼Œä½¿ç”¨è½¬å½•: '\(currentAnalysisTranscription)'")
                    analysisCompletionHandler?(summary, currentAnalysisTranscription, audioType)
                }
            }
            
        case "response.content_part.done":
            // Alternative completion signal - WAIT for transcription before completing
            if let part = json["part"] as? [String: Any],
               let transcript = part["transcript"] as? String {
                // Clean EOT marker immediately
                let cleanTranscript = transcript
                    .replacingOccurrences(of: "<|EOT|>", with: "")
                    .replacingOccurrences(of: "<|eot|>", with: "")
                    .trimmingCharacters(in: .whitespacesAndNewlines)
                
                print("ğŸ¤– AIå†…å®¹å®Œæˆ: \(cleanTranscript)")
                print("ğŸ¤– AIåŸå§‹å›å¤: \(transcript)")
                let (summary, audioType) = parseAIResponse(cleanTranscript)
                
                print("âœ… AIåˆ†æå®Œæˆ: æ€»ç»“=\(summary), ç±»å‹=\(audioType.displayName)")
                print("ğŸ” å½“å‰åˆ†æè½¬å½•: '\(currentAnalysisTranscription)'")
                
                // ğŸ”§ CRITICAL FIX: Wait for transcription if not yet received
                if currentAnalysisTranscription.isEmpty {
                    print("â° AIå®Œæˆä½†è½¬å½•æœªæ”¶åˆ°ï¼Œå»¶è¿Ÿ3ç§’ç­‰å¾…è½¬å½•...")
                    DispatchQueue.global().asyncAfter(deadline: .now() + 3.0) {
                        let finalTranscription = self.currentAnalysisTranscription.isEmpty ? cleanTranscript : self.currentAnalysisTranscription
                        print("ğŸ”§ å»¶è¿Ÿå®Œæˆï¼Œä½¿ç”¨è½¬å½•: '\(finalTranscription)'")
                        self.analysisCompletionHandler?(summary, finalTranscription, audioType)
                    }
                } else {
                    // Transcription already received, complete immediately
                    print("ğŸ”§ ç«‹å³å®Œæˆï¼Œä½¿ç”¨è½¬å½•: '\(currentAnalysisTranscription)'")
                    analysisCompletionHandler?(summary, currentAnalysisTranscription, audioType)
                }
            }
            
        default:
            break
        }
    }
    
    // Fallback summary generation
    private func generateFallbackSummary(duration: Int) async -> (summary: String, transcription: String, audioType: AudioType) {
        print("ğŸ”„ ç”Ÿæˆfallbackæ™ºèƒ½æ‘˜è¦...")
        
        // åŸºäºæ—¶é•¿å’Œæ—¶é—´çš„æ™ºèƒ½æ¨æµ‹ - ä½¿ç”¨emoji:summaryæ ¼å¼
        let summary = generateSmartSummaryWithEmoji(duration: duration)
        let transcription = "éŸ³é¢‘å†…å®¹è®°å½• (\(duration)ç§’)"
        let audioType = inferAudioTypeFromDuration(duration: duration)
        
        print("ğŸ”„ Fallbackç»“æœ: \(summary) | ç±»å‹: \(audioType.displayName)")
        return (summary, transcription, audioType)
    }
    
    // Generate emoji-based smart summary for fallback
    private func generateSmartSummaryWithEmoji(duration: Int) -> String {
        let hour = Calendar.current.component(.hour, from: Date())
        let timeOfDay = getTimeOfDay(hour: hour)
        let durationCategory = getDurationCategory(duration: duration)
        
        // åŸºäºæ—¶é—´å’Œæ—¶é•¿çš„æ™ºèƒ½emojié€‰æ‹©
        let emojiSummaries = [
            "ğŸµ:\(timeOfDay)æ­Œå£°",      // å¯èƒ½æ˜¯å”±æ­Œ
            "ğŸ’¬:\(timeOfDay)å¯¹è¯",      // å¯èƒ½æ˜¯å¯¹è¯
            "ğŸ“:\(durationCategory)å½•éŸ³", // ä¸€èˆ¬å½•éŸ³
            "ğŸ¤:\(timeOfDay)è¯­éŸ³",      // è¯­éŸ³å†…å®¹
            "ğŸ“:\(timeOfDay)é€šè¯",      // å¯èƒ½æ˜¯é€šè¯
            "ğŸ¢:\(timeOfDay)ä¼šè®®",      // å·¥ä½œä¼šè®®
            "ğŸ˜Š:\(timeOfDay)èŠå¤©",      // è½»æ¾èŠå¤©
        ]
        
        return emojiSummaries.randomElement() ?? "ğŸ“:\(durationCategory)å½•éŸ³"
    }
    
    // Generate context-aware summary
    private func generateSmartSummary(duration: Int) -> String {
        let hour = Calendar.current.component(.hour, from: Date())
        let timeOfDay = getTimeOfDay(hour: hour)
        let durationCategory = getDurationCategory(duration: duration)
        
        let summaries = [
            "\(timeOfDay)\(durationCategory)",
            "å½•éŸ³å†…å®¹",
            "\(timeOfDay)è®°å½•",
            "è¯­éŸ³å¤‡å¿˜",
            "\(durationCategory)éŸ³é¢‘",
            "ä¸ªäººå½•éŸ³",
            "é‡è¦å†…å®¹",
            "è¯­éŸ³ç¬”è®°"
        ]
        
        return summaries.randomElement() ?? "å½•éŸ³"
    }
    
    private func getTimeOfDay(hour: Int) -> String {
        switch hour {
        case 6..<9: return "æ™¨é—´"
        case 9..<12: return "ä¸Šåˆ"
        case 12..<14: return "åˆé—´" 
        case 14..<18: return "ä¸‹åˆ"
        case 18..<22: return "æ™šé—´"
        default: return "å¤œé—´"
        }
    }
    
    private func getDurationCategory(duration: Int) -> String {
        switch duration {
        case 0..<30: return "ç®€çŸ­"
        case 30..<180: return "å¸¸è§„"
        case 180..<600: return "è¯¦ç»†"
        default: return "é•¿ç¯‡"
        }
    }
    
    private func inferAudioTypeFromDuration(duration: Int) -> AudioType {
        switch duration {
        case 0..<15: return .humanVoice
        case 15..<60: return .conversation
        case 60..<300: return .humanVoice
        default: return .conversation
        }
    }
    
    // Original HTTP analysis (currently disabled due to model unavailability)
    private func analyzeAudioWithStepAPI(audioURL: String, duration: Int) async throws -> (summary: String, transcription: String, audioType: AudioType) {
        print("ğŸ¯ ä½¿ç”¨Step APIåˆ†æéŸ³é¢‘...")
        
        guard let url = URL(string: "https://api.stepfun.com/v1/chat/completions") else {
            throw NSError(domain: "StepAPI", code: 1, userInfo: [NSLocalizedDescriptionKey: "Invalid API URL"])
        }
        
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        
        let prompt = """
        è¯·åŸºäºéŸ³é¢‘æ—¶é•¿(\(duration)ç§’)ç”Ÿæˆæ™ºèƒ½æ€»ç»“ã€‚
        
        å›å¤æ ¼å¼ï¼š
        è½¬å½•ï¼šéŸ³é¢‘å†…å®¹è®°å½•
        æ€»ç»“ï¼š[5-8å­—æ™ºèƒ½æ€»ç»“]
        ç±»å‹ï¼šhuman_voice
        """
        
        let requestBody: [String: Any] = [
            "model": "step-1v-8k",
            "messages": [
                [
                    "role": "user",
                    "content": prompt
                ]
            ],
            "max_tokens": 100,
            "temperature": 0.7
        ]
        
        do {
            request.httpBody = try JSONSerialization.data(withJSONObject: requestBody)
            let (data, response) = try await URLSession.shared.data(for: request)
            
            if let httpResponse = response as? HTTPURLResponse {
                print("ğŸ“¥ Step Audio APIå“åº”ç : \(httpResponse.statusCode)")
                
                if httpResponse.statusCode != 200 {
                    // Log response body for debugging
                    if let responseString = String(data: data, encoding: .utf8) {
                        print("ğŸ“¥ Error response: \(responseString)")
                    }
                    throw NSError(domain: "StepAudio", code: httpResponse.statusCode,
                                userInfo: [NSLocalizedDescriptionKey: "HTTP \(httpResponse.statusCode)"])
                }
            }
            
            let json = try JSONSerialization.jsonObject(with: data) as? [String: Any]
            
            if let choices = json?["choices"] as? [[String: Any]],
               let firstChoice = choices.first,
               let message = firstChoice["message"] as? [String: Any],
               let content = message["content"] as? String {
                
                print("ğŸ¤– AIåŸå§‹å›å¤: \(content)")
                
                // Parse the structured response
                let result = parseMultimodalResponse(content)
                print("âœ… å¤šæ¨¡æ€åˆ†æå®Œæˆ: æ€»ç»“=\(result.summary), ç±»å‹=\(result.audioType.displayName)")
                return (result.summary, result.transcription, result.audioType)
            }
            
            // Fallback if parsing fails
            return ("éŸ³é¢‘åˆ†æ", "è½¬å½•å¤±è´¥", .humanVoice)
            
        } catch {
            print("âŒ Step Audio APIè°ƒç”¨å¤±è´¥: \(error)")
            throw error
        }
    }
    
    // Parse multimodal response
    private func parseMultimodalResponse(_ content: String) -> (summary: String, transcription: String, audioType: AudioType) {
        var transcription = "è½¬å½•å¤±è´¥"
        var summary = "éŸ³é¢‘åˆ†æ"
        var audioType = AudioType.humanVoice
        
        let lines = content.components(separatedBy: "\n")
        
        for line in lines {
            if line.contains("è½¬å½•ï¼š") {
                transcription = line.replacingOccurrences(of: "è½¬å½•ï¼š", with: "").trimmingCharacters(in: .whitespacesAndNewlines)
            } else if line.contains("æ€»ç»“ï¼š") {
                summary = line.replacingOccurrences(of: "æ€»ç»“ï¼š", with: "").trimmingCharacters(in: .whitespacesAndNewlines)
            } else if line.contains("ç±»å‹ï¼š") {
                let typeString = line.replacingOccurrences(of: "ç±»å‹ï¼š", with: "").trimmingCharacters(in: .whitespacesAndNewlines)
                audioType = AudioType(rawValue: typeString) ?? .humanVoice
            }
        }
        
        return (summary, transcription, audioType)
    }
    
    func connect() {
        isManualDisconnect = false
        attemptConnection()
    }
    
    private func attemptConnection() {
        // Prevent multiple connection attempts
        if connectionStatus == .connecting || connectionStatus == .connected {
            print("âš ï¸ WebSocket already connecting or connected")
            return
        }
        
        // Check circuit breaker
        if circuitBreakerOpen {
            if let lastFailure = lastConnectionFailure,
               Date().timeIntervalSince(lastFailure) < circuitBreakerTimeout {
                print("âš¡ Circuit breaker open, skipping connection attempt")
                scheduleReconnect()
                return
            } else {
                print("âš¡ Circuit breaker timeout elapsed, attempting connection")
                circuitBreakerOpen = false
                consecutiveFailures = 0
            }
        }
        
        updateConnectionStatus(.connecting)
        
        // Model parameter is REQUIRED in URL for Step API
        let urlWithModel = "\(wsURL)?model=step-audio-2-mini"
        guard let url = URL(string: urlWithModel) else {
            print("âŒ Invalid WebSocket URL: \(urlWithModel)")
            updateConnectionStatus(.failed)
            return
        }
        
        var request = URLRequest(url: url)
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        request.timeoutInterval = 30.0  // Shorter timeout for faster failure detection
        
        // Use minimal standard WebSocket headers
        request.setValue("websocket", forHTTPHeaderField: "Upgrade")
        request.setValue("Upgrade", forHTTPHeaderField: "Connection")
        request.setValue("13", forHTTPHeaderField: "Sec-WebSocket-Version")
        
        // Add Step API specific headers
        request.setValue("step-audio-2-mini", forHTTPHeaderField: "X-Model")
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        
        print("ğŸ”Œ Attempting to connect to: \(urlWithModel) (Attempt: \(reconnectAttempts + 1))")
        print("ğŸ”‘ Using API key: \(apiKey.prefix(10))...")
        print("ğŸ“‹ Model: step-audio-2-mini")
        
        // Cleanup previous connection
        cleanupConnection()
        
        webSocketTask = urlSession.webSocketTask(with: request)
        webSocketTask?.resume()
        
        // Start receiving messages
        receiveMessage()
        
        // Start connection health monitoring
        startConnectionHealthMonitoring()
        
        // Set connection timeout
        Task {
            try await Task.sleep(nanoseconds: 10_000_000_000) // 10 seconds timeout
            if connectionStatus == .connecting {
                print("âŒ Connection timeout after 10s")
                handleConnectionFailure()
            }
        }
    }
    
    // è¿æ¥çŠ¶æ€æ›´æ–°
    private func updateConnectionStatus(_ status: ConnectionStatus) {
        DispatchQueue.main.async {
            self.connectionStatus = status
            self.isConnected = (status == .connected)
            print("ğŸ”„ Connection status: \(status.rawValue)")
        }
    }
    
    // è¿æ¥å¥åº·ç›‘æ§
    private func startConnectionHealthMonitoring() {
        connectionHealthTimer?.invalidate()
        connectionHealthTimer = Timer.scheduledTimer(withTimeInterval: 15.0, repeats: true) { [weak self] _ in
            self?.checkConnectionHealth()
        }
    }
    
    // æ£€æŸ¥è¿æ¥å¥åº·çŠ¶æ€
    private func checkConnectionHealth() {
        guard let task = webSocketTask else { return }
        
        if task.state != .running {
            print("âš ï¸ WebSocket connection unhealthy, state: \(task.state.rawValue)")
            handleConnectionFailure()
        } else {
            // Send ping to verify connection is truly alive
            task.sendPing { [weak self] error in
                if let error = error {
                    print("âŒ Health check ping failed: \(error)")
                    self?.handleConnectionFailure()
                } else {
                    print("âœ… Connection health check passed")
                }
            }
        }
    }
    
    // å¤„ç†è¿æ¥å¤±è´¥
    private func handleConnectionFailure() {
        guard !isManualDisconnect else { return }
        
        print("âŒ Handling connection failure")
        cleanupConnection()
        
        // Update failure tracking
        lastConnectionFailure = Date()
        consecutiveFailures += 1
        
        // Open circuit breaker after too many consecutive failures
        if consecutiveFailures >= 3 {
            print("âš¡ Opening circuit breaker after \(consecutiveFailures) consecutive failures")
            circuitBreakerOpen = true
        }
        
        if reconnectAttempts < maxReconnectAttempts {
            scheduleReconnect()
        } else {
            print("âŒ Max reconnect attempts reached")
            updateConnectionStatus(.failed)
            resetReconnectAttempts()
            
            // Extended delay before allowing new connection attempts
            Task {
                try? await Task.sleep(nanoseconds: 30_000_000_000) // 30 seconds
                print("ğŸ”„ Resetting connection failure state")
                consecutiveFailures = 0
                circuitBreakerOpen = false
            }
        }
    }
    
    // å®‰æ’é‡è¿
    private func scheduleReconnect() {
        reconnectAttempts += 1
        updateConnectionStatus(.reconnecting)
        
        let delay = min(reconnectInterval * Double(reconnectAttempts), 30.0) // Max 30 seconds
        print("ğŸ”„ Scheduling reconnect in \(delay) seconds (attempt \(reconnectAttempts))")
        
        reconnectTimer?.invalidate()
        reconnectTimer = Timer.scheduledTimer(withTimeInterval: delay, repeats: false) { [weak self] _ in
            self?.attemptConnection()
        }
    }
    
    // é‡ç½®é‡è¿å°è¯•è®¡æ•°
    private func resetReconnectAttempts() {
        reconnectAttempts = 0
        reconnectTimer?.invalidate()
        reconnectTimer = nil
    }
    
    // æ¸…ç†è¿æ¥èµ„æº
    private func cleanupConnection() {
        pingTimer?.invalidate()
        pingTimer = nil
        
        webSocketTask?.cancel()
        webSocketTask = nil
    }
    
    private func reconnectWithDelay() async {
        handleConnectionFailure()
    }
    
    private func testAPIConnection() async {
        print("ğŸ” Testing API connectivity...")
        
        guard let testURL = URL(string: "https://api.stepfun.com/v1/chat/completions") else { return }
        
        var request = URLRequest(url: testURL)
        request.httpMethod = "POST"
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        
        let testBody: [String: Any] = [
            "model": "step-1v-8k",
            "messages": [["role": "user", "content": "test"]],
            "max_tokens": 10
        ]
        
        do {
            request.httpBody = try JSONSerialization.data(withJSONObject: testBody)
            let (_, response) = try await URLSession.shared.data(for: request)
            
            if let httpResponse = response as? HTTPURLResponse {
                print("ğŸ” API Test Response Code: \(httpResponse.statusCode)")
                if httpResponse.statusCode == 401 {
                    print("âŒ API Key è®¤è¯å¤±è´¥ - å¯èƒ½æ˜¯æ— æ•ˆçš„API Key")
                } else if httpResponse.statusCode == 200 {
                    print("âœ… API Key è®¤è¯æˆåŠŸ")
                }
            }
        } catch {
            print("âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥: \(error)")
        }
    }
    
    func disconnect() {
        print("ğŸ”Œ Manual disconnect requested")
        isManualDisconnect = true
        
        // Stop all timers
        reconnectTimer?.invalidate()
        pingTimer?.invalidate()
        connectionHealthTimer?.invalidate()
        
        // Cancel WebSocket connection
        webSocketTask?.cancel(with: .goingAway, reason: nil)
        webSocketTask = nil
        
        // Update status
        updateConnectionStatus(.disconnected)
        
        // Stop recording if active
        stopRecording()
        
        // Reset reconnect attempts
        resetReconnectAttempts()
    }
    
    // MARK: - Sessioné…ç½®
    
    private func configureSession() async {
        let sessionConfig: [String: Any] = [
            "event_id": UUID().uuidString,
            "type": "session.update",
            "session": [
                "model": "step-audio-2-mini", 
                "modalities": ["text", "audio"],
                "instructions": """
                åˆ†æéŸ³é¢‘ï¼Œåªè¾“å‡ºï¼š
                emoji:summary

                è§„åˆ™ï¼š
                1. emojiåªèƒ½1ä¸ª
                2. summaryé™5-8ä¸ªå­—
                3. ä¸è¦åŠ ä»»ä½•æ ‡è®°æˆ–ç¬¦å·
                4. å”±æ­Œä¼˜å…ˆè¯†åˆ«ä¸ºğŸµ
                5. ä¸è¦è¾“å‡º<|EOT|>æˆ–å…¶ä»–ç»“æŸæ ‡è®°
                
                ç¤ºä¾‹ï¼š
                ğŸµ:å†›æ­Œå˜¹äº®
                ğŸ˜Š:å¼€å¿ƒèŠå¤©
                ğŸŒ§ï¸:é›¨å£°ç¯å¢ƒ
                """,
                "voice": "qingchunshaonv",
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "turn_detection": NSNull()
            ]
        ]
        
        await sendMessage(sessionConfig)
    }
    
    // MARK: - æ¶ˆæ¯å‘é€ä¸æ¥æ”¶
    
    private func sendMessage(_ message: [String: Any], retryCount: Int = 0) async {
        guard let webSocketTask = webSocketTask else {
            print("âŒ WebSocketæœªè¿æ¥ï¼Œæ— æ³•å‘é€æ¶ˆæ¯")
            if !isManualDisconnect && connectionStatus != .connecting {
                handleConnectionFailure()
            }
            return
        }
        
        // Check if WebSocket is in a valid state
        guard webSocketTask.state == .running else {
            print("âŒ WebSocketçŠ¶æ€æ— æ•ˆ: \(webSocketTask.state.rawValue)")
            if !isManualDisconnect {
                handleConnectionFailure()
            }
            return
        }
        
        guard let jsonData = try? JSONSerialization.data(withJSONObject: message),
              let jsonString = String(data: jsonData, encoding: .utf8) else {
            print("âŒ æ¶ˆæ¯åºåˆ—åŒ–å¤±è´¥")
            return
        }
        
        // Debug: æ‰“å°å‘é€çš„æ¶ˆæ¯
        if let type = message["type"] as? String, type != "input_audio_buffer.append" {
            print("ğŸ“¤ å‘é€æ¶ˆæ¯: \(type)")
            if retryCount > 0 {
                print("ğŸ“¤ é‡è¯•å‘é€ (ç¬¬\(retryCount + 1)æ¬¡)")
            }
        }
        
        let wsMessage = URLSessionWebSocketTask.Message.string(jsonString)
        
        do {
            try await webSocketTask.send(wsMessage)
        } catch {
            print("âŒ å‘é€æ¶ˆæ¯å¤±è´¥: \(error)")
            
            // Check if we should retry
            if retryCount < 3 && !isManualDisconnect {
                print("ğŸ”„ å°†åœ¨1ç§’åé‡è¯•å‘é€æ¶ˆæ¯")
                try? await Task.sleep(nanoseconds: 1_000_000_000)
                await sendMessage(message, retryCount: retryCount + 1)
            } else {
                // Max retries reached or connection issues
                if !isManualDisconnect {
                    print("âŒ æ¶ˆæ¯å‘é€å¤±è´¥ï¼Œè§¦å‘é‡è¿")
                    handleConnectionFailure()
                }
            }
        }
    }
    
    private func receiveMessage() {
        webSocketTask?.receive { [weak self] result in
            switch result {
            case .success(let message):
                switch message {
                case .string(let text):
                    self?.handleServerEvent(text)
                case .data(let data):
                    if let text = String(data: data, encoding: .utf8) {
                        self?.handleServerEvent(text)
                    }
                @unknown default:
                    break
                }
                
                // ç»§ç»­æ¥æ”¶ä¸‹ä¸€æ¡æ¶ˆæ¯
                self?.receiveMessage()
                
            case .failure(let error):
                print("âŒ æ¥æ”¶æ¶ˆæ¯å¤±è´¥: \(error)")
                
                // Check if this is a connection issue that warrants reconnection
                if let urlError = error as? URLError {
                    switch urlError.code {
                    case .networkConnectionLost, .timedOut, .notConnectedToInternet:
                        print("ğŸŒ Network error while receiving, will attempt reconnect")
                        self?.handleConnectionFailure()
                        return
                    default:
                        break
                    }
                }
                
                // For other errors, also attempt reconnection if not manually disconnected
                if let strongSelf = self, !strongSelf.isManualDisconnect {
                    print("ğŸ”„ Receive error, attempting reconnect")
                    strongSelf.handleConnectionFailure()
                }
            }
        }
    }
    
    private func handleServerEvent(_ jsonString: String) {
        print("ğŸ“¥ æ”¶åˆ°æœåŠ¡å™¨æ¶ˆæ¯: \(jsonString)")
        
        guard let data = jsonString.data(using: .utf8),
              let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
              let type = json["type"] as? String else { 
            print("âŒ æ— æ³•è§£ææœåŠ¡å™¨æ¶ˆæ¯")
            return 
        }
        
        DispatchQueue.main.async { [weak self] in
            switch type {
            case "session.created":
                self?.isConnected = true
                print("âœ… Sessionå·²åˆ›å»º")
                
            case "session.updated":
                self?.isConnected = true  
                print("âœ… Sessionå·²æ›´æ–°")
                
            case "input_audio_buffer.speech_started":
                // VADæ£€æµ‹åˆ°è¯´è¯å¼€å§‹
                print("ğŸ¤ æ£€æµ‹åˆ°éŸ³é¢‘è¾“å…¥å¼€å§‹")
                self?.isProcessing = true
                self?.processingStartTime = Date()
                
            case "input_audio_buffer.speech_stopped":
                // VADæ£€æµ‹åˆ°è¯´è¯ç»“æŸ
                print("ğŸ”‡ æ£€æµ‹åˆ°è¯´è¯ç»“æŸ")
                
            case "conversation.item.input_audio_transcription.completed":
                // ç”¨æˆ·éŸ³é¢‘è½¬å½•å®Œæˆ
                if let transcript = json["transcript"] as? String {
                    self?.currentTranscription = transcript
                    self?.onTranscriptionUpdate?(transcript)
                    
                    // è§¦å‘æ™ºèƒ½åˆ†æå’Œæ€»ç»“
                    self?.requestSmartAnalysis(for: transcript)
                }
                
            case "response.audio_transcript.delta":
                // AIå›å¤çš„æ–‡å­—æµï¼ˆåˆ†æç»“æœï¼‰
                if let delta = json["delta"] as? String {
                    self?.currentSummary += delta
                }
                
            case "response.audio_transcript.done":
                // AIå›å¤å®Œæˆ
                if let transcript = json["transcript"] as? String {
                    // Clean EOT marker immediately
                    let cleanTranscript = transcript
                        .replacingOccurrences(of: "<|EOT|>", with: "")
                        .replacingOccurrences(of: "<|eot|>", with: "")
                        .trimmingCharacters(in: .whitespacesAndNewlines)
                    
                    print("ğŸ¤ å®æ—¶å½•éŸ³AIå›å¤: \(cleanTranscript)")
                    let (summary, detectedType) = self?.parseAIResponse(cleanTranscript) ?? ("", .unknown)
                    
                    print("ğŸ¤ å®æ—¶åˆ†æç»“æœ: æ€»ç»“=\(summary), ç±»å‹=\(detectedType.displayName)")
                    
                    self?.currentSummary = summary
                    self?.audioType = detectedType
                    self?.isProcessing = false
                    
                    // è®¡ç®—å¤„ç†æ—¶é—´
                    let processingTime = Date().timeIntervalSince(self?.processingStartTime ?? Date())
                    print("â±ï¸ å¤„ç†è€—æ—¶: \(String(format: "%.1f", processingTime))ç§’")
                    
                    self?.onSummaryGenerated?(summary, detectedType)
                    self?.onAudioTypeDetected?(detectedType)
                    
                    // ä¿å­˜åˆ°å†å²
                    let item = ConversationItem(
                        transcription: self?.currentTranscription ?? "",
                        summary: summary,
                        timestamp: Date(),
                        audioType: detectedType.rawValue
                    )
                    self?.conversationHistory.append(item)
                }
                
            case "error":
                if let error = json["error"] as? [String: Any],
                   let message = error["message"] as? String {
                    print("âŒ æœåŠ¡å™¨é”™è¯¯: \(message)")
                }
                
            default:
                break
            }
        }
    }
    
    // MARK: - éŸ³é¢‘å½•åˆ¶ä¸å‘é€
    
    func startRecording() {
        guard !isRecording else { return }
        
        setupAudioSession()
        
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, _ in
            self?.processAudioBuffer(buffer)
        }
        
        do {
            try audioEngine.start()
            isRecording = true
            print("ğŸ¤ å¼€å§‹å½•éŸ³")
        } catch {
            print("âŒ å¯åŠ¨å½•éŸ³å¤±è´¥: \(error)")
        }
    }
    
    func stopRecording() {
        guard isRecording else { return }
        
        do {
            audioEngine.stop()
            inputNode.removeTap(onBus: 0)
            
            // Reset audio session
            try AVAudioSession.sharedInstance().setActive(false)
        } catch {
            print("âŒ åœæ­¢å½•éŸ³æ—¶éŸ³é¢‘ä¼šè¯é‡ç½®å¤±è´¥: \(error)")
        }
        
        isRecording = false
        
        // æäº¤éŸ³é¢‘ç¼“å†²åŒº
        Task {
            await commitAudioBuffer()
        }
        
        print("ğŸ›‘ åœæ­¢å½•éŸ³")
    }
    
    private func setupAudioSession() {
        let session = AVAudioSession.sharedInstance()
        
        do {
            try session.setCategory(.playAndRecord, mode: .default)
            try session.setActive(true)
        } catch {
            print("âŒ éŸ³é¢‘ä¼šè¯è®¾ç½®å¤±è´¥: \(error)")
        }
    }
    
    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer) {
        // è½¬æ¢ä¸ºPCM16æ ¼å¼
        guard let pcm16Data = convertToPCM16(buffer: buffer) else { return }
        
        // Base64ç¼–ç 
        let base64Audio = pcm16Data.base64EncodedString()
        
        // å‘é€åˆ°æœåŠ¡å™¨
        Task {
            let message: [String: Any] = [
                "event_id": UUID().uuidString,
                "type": "input_audio_buffer.append",
                "audio": base64Audio
            ]
            await sendMessage(message)
        }
    }
    
    private func convertToPCM16(buffer: AVAudioPCMBuffer) -> Data? {
        guard let channelData = buffer.int16ChannelData else {
            // å¦‚æœä¸æ˜¯int16æ ¼å¼ï¼Œéœ€è¦è½¬æ¢
            return convertFloatToPCM16(buffer: buffer)
        }
        
        let channelCount = Int(buffer.format.channelCount)
        let frameLength = Int(buffer.frameLength)
        
        var data = Data()
        for frame in 0..<frameLength {
            for channel in 0..<channelCount {
                let sample = channelData[channel][frame]
                data.append(contentsOf: withUnsafeBytes(of: sample) { Array($0) })
            }
        }
        
        return data
    }
    
    private func convertFloatToPCM16(buffer: AVAudioPCMBuffer) -> Data? {
        guard let channelData = buffer.floatChannelData else { return nil }
        
        let channelCount = Int(buffer.format.channelCount)
        let frameLength = Int(buffer.frameLength)
        
        var data = Data()
        for frame in 0..<frameLength {
            for channel in 0..<channelCount {
                let floatSample = channelData[channel][frame]
                let int16Sample = Int16(max(-32768, min(32767, floatSample * 32768)))
                data.append(contentsOf: withUnsafeBytes(of: int16Sample) { Array($0) })
            }
        }
        
        return data
    }
    
    private func commitAudioBuffer() async {
        let message: [String: Any] = [
            "event_id": UUID().uuidString,
            "type": "input_audio_buffer.commit"
        ]
        await sendMessage(message)
    }
    
    // MARK: - æ™ºèƒ½åˆ†æåŠŸèƒ½
    
    private func requestSmartAnalysis(for text: String) {
        Task {
            // å¦‚æœæ˜¯ç¯å¢ƒéŸ³æˆ–æ— è½¬å½•å†…å®¹ï¼Œç›´æ¥é€šè¿‡VADå¤„ç†
            if text.isEmpty || text.count < 3 {
                await requestEnvironmentAnalysis()
                return
            }
            
            // æœ‰å†…å®¹åˆ™è¿›è¡Œæ™ºèƒ½åˆ†æ
            let message: [String: Any] = [
                "event_id": UUID().uuidString,
                "type": "conversation.item.create", 
                "item": [
                    "id": UUID().uuidString,
                    "type": "message",
                    "role": "user",
                    "content": [
                        [
                            "type": "input_text",
                            "text": "è¯·æ ¹æ®ä»¥ä¸‹éŸ³é¢‘è½¬å½•å†…å®¹ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼Œè¦æ±‚ï¼š1)å‡†ç¡®åæ˜ æ ¸å¿ƒå†…å®¹ï¼Œ2)ä¸è¶…è¿‡8ä¸ªå­—ï¼Œ3)é¿å…ä½¿ç”¨emojiæˆ–æ— å…³è£…é¥°æ–‡å­—ï¼Œ4)ç›´æ¥æå–å…³é”®ä¿¡æ¯ã€‚è½¬å½•å†…å®¹ï¼š\(text)"
                        ]
                    ]
                ]
            ]
            await sendMessage(message)
            
            // è§¦å‘æ¨ç†
            let responseMessage: [String: Any] = [
                "event_id": UUID().uuidString,
                "type": "response.create"
            ]
            await sendMessage(responseMessage)
        }
    }
    
    private func requestEnvironmentAnalysis() async {
        let message: [String: Any] = [
            "event_id": UUID().uuidString,
            "type": "conversation.item.create",
            "item": [
                "id": UUID().uuidString,
                "type": "message", 
                "role": "user",
                "content": [
                    [
                        "type": "input_text",
                        "text": "åˆšæ‰æ£€æµ‹åˆ°ç¯å¢ƒéŸ³é¢‘ï¼Œè¯·è¯†åˆ«å£°éŸ³ç±»å‹"
                    ]
                ]
            ]
        ]
        await sendMessage(message)
        
        let responseMessage: [String: Any] = [
            "event_id": UUID().uuidString,
            "type": "response.create"
        ]
        await sendMessage(responseMessage)
    }
    
    // æ£€æµ‹å­—ç¬¦ä¸²æ˜¯å¦åŒ…å«emoji
    private func containsEmoji(_ string: String) -> Bool {
        return string.unicodeScalars.contains { scalar in
            scalar.properties.isEmoji
        }
    }
    
    private func parseAIResponse(_ response: String) -> (String, AudioType) {
        print("ğŸ¤– AIåŸå§‹å›å¤: \(response)")
        print("ğŸ” å“åº”é•¿åº¦: \(response.count) å­—ç¬¦")
        
        // Clean up common artifacts and trailing garbage
        var cleanResponse = response
            .replacingOccurrences(of: "<|EOT|>", with: "")
            .replacingOccurrences(of: "<|eot|>", with: "")
            .replacingOccurrences(of: "\\n", with: " ")
            .replacingOccurrences(of: "\n\n", with: " ")
            .trimmingCharacters(in: .whitespacesAndNewlines)
        
        // Remove everything after common end markers
        if let range = cleanResponse.range(of: "â†’") {
            // Keep everything including the arrow but remove anything after the next word
            let afterArrow = String(cleanResponse[range.upperBound...])
            if let spaceIndex = afterArrow.firstIndex(of: " ") {
                cleanResponse = String(cleanResponse[..<range.upperBound]) + String(afterArrow[..<spaceIndex])
            }
        }
        
        // Parse emoji:summary format
        if cleanResponse.contains(":") {
            print("ğŸ“‹ æ£€æµ‹åˆ°emoji:summaryæ ¼å¼")
            
            // Handle "emoji:" prefix if present
            if cleanResponse.lowercased().hasPrefix("emoji:") {
                cleanResponse = String(cleanResponse.dropFirst(6)).trimmingCharacters(in: .whitespacesAndNewlines)
            }
            
            // Find first colon to split emoji and summary
            if let colonIndex = cleanResponse.firstIndex(of: ":") {
                let emojiPart = String(cleanResponse[..<colonIndex]).trimmingCharacters(in: .whitespacesAndNewlines)
                var summaryPart = String(cleanResponse[cleanResponse.index(after: colonIndex)...]).trimmingCharacters(in: .whitespacesAndNewlines)
                
                // Clean summary - remove any trailing garbage after 8 characters
                if summaryPart.count > 8 {
                    // Keep only first 8 characters for summary
                    let endIndex = summaryPart.index(summaryPart.startIndex, offsetBy: 8)
                    summaryPart = String(summaryPart[..<endIndex])
                }
                
                // Validate emoji (should be 1-2 characters max)
                if emojiPart.count <= 2 && containsEmoji(emojiPart) && !summaryPart.isEmpty {
                    let fullContent = "\(emojiPart):\(summaryPart)"
                    print("ğŸ“‹ è§£æemoji:summaryå†…å®¹: \(fullContent)")
                    
                    // Determine audio type from emoji and content
                    let audioType = inferAudioTypeFromEmojiContent(emoji: emojiPart, content: summaryPart)
                    
                    return (fullContent, audioType)
                }
            }
            
            // ä¹Ÿæ£€æŸ¥å…¶ä»–emojiæ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            let lines = cleanResponse.components(separatedBy: "\n")
            for line in lines {
                let trimmedLine = line.trimmingCharacters(in: .whitespacesAndNewlines)
                
                // æ£€æŸ¥æ˜¯å¦åŒ…å«emojiå’Œå†’å·
                if trimmedLine.contains(":") && trimmedLine.count > 2 {
                    // å°è¯•åˆ†å‰²emojiå’Œsummary
                    let parts = trimmedLine.components(separatedBy: ":")
                    if parts.count >= 2 {
                        let emojiPart = parts[0].trimmingCharacters(in: .whitespacesAndNewlines)
                        let summaryPart = parts[1].trimmingCharacters(in: .whitespacesAndNewlines)
                        
                        // éªŒè¯ç¬¬ä¸€éƒ¨åˆ†æ˜¯å¦åŒ…å«emojiï¼ˆæ›´å®½æ¾çš„æ£€æŸ¥ï¼‰
                        if emojiPart.count <= 4 && !summaryPart.isEmpty && containsEmoji(emojiPart) {
                            let fullContent = "\(emojiPart):\(summaryPart)"
                            print("ğŸ“‹ è§£æemoji:summaryå†…å®¹: \(fullContent)")
                            
                            // æ ¹æ®emojiå’Œå†…å®¹ç¡®å®šéŸ³é¢‘ç±»å‹
                            let audioType = inferAudioTypeFromEmojiContent(emoji: emojiPart, content: summaryPart)
                            
                            return (fullContent, audioType)
                        }
                    }
                }
            }
            
            print("âš ï¸ emoji:summaryæ ¼å¼ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆå†…å®¹ï¼Œç»§ç»­å…¶ä»–æ ¼å¼è§£æ")
        }
        
        // è§£ææ—§çš„çŠ¶æ€æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
        if response.contains("çŠ¶æ€ï¼š") || response.contains("çŠ¶æ€:") {
            print("ğŸ“‹ æ£€æµ‹åˆ°æ—§çŠ¶æ€æ ¼å¼")
            
            // æå–çŠ¶æ€è¡Œå†…å®¹
            let lines = response.components(separatedBy: "\n")
            for line in lines {
                let trimmedLine = line.trimmingCharacters(in: .whitespacesAndNewlines)
                
                if trimmedLine.contains("çŠ¶æ€ï¼š") || trimmedLine.contains("çŠ¶æ€:") {
                    let statusContent = trimmedLine.replacingOccurrences(of: "çŠ¶æ€ï¼š", with: "")
                        .replacingOccurrences(of: "çŠ¶æ€:", with: "")
                        .trimmingCharacters(in: .whitespacesAndNewlines)
                    
                    print("ğŸ“‹ è§£ææ—§çŠ¶æ€å†…å®¹: \(statusContent)")
                    
                    // æ ¹æ®çŠ¶æ€å†…å®¹ç¡®å®šéŸ³é¢‘ç±»å‹
                    let audioType = inferAudioTypeFromStatus(statusContent)
                    
                    // ç›´æ¥ä½¿ç”¨çŠ¶æ€å†…å®¹ä½œä¸ºæ€»ç»“
                    return (statusContent, audioType)
                }
            }
            
            print("âš ï¸ æ—§çŠ¶æ€æ ¼å¼ä¸­æœªæ‰¾åˆ°çŠ¶æ€è¡Œï¼Œç»§ç»­å…¶ä»–æ ¼å¼è§£æ")
        }
        
        // è§£ææ—§çš„ç»“æ„åŒ–è¾“å‡ºæ ¼å¼  
        if response.contains("äººå£°å†…å®¹:") {
            print("ğŸ“‹ æ£€æµ‹åˆ°ç»“æ„åŒ–äººå£°å†…å®¹æ ¼å¼")
            // äººå£°å†…å®¹è§£æ
            var summary = "è¯­éŸ³å†…å®¹"
            var expressionType = ""
            var emotion = ""
            var characteristics = ""
            
            let lines = response.components(separatedBy: "\n")
            for line in lines {
                let trimmedLine = line.trimmingCharacters(in: .whitespacesAndNewlines)
                
                if trimmedLine.contains("æ€»ç»“ï¼š") {
                    summary = trimmedLine.replacingOccurrences(of: "- æ€»ç»“ï¼š", with: "")
                        .replacingOccurrences(of: "æ€»ç»“ï¼š", with: "")
                        .trimmingCharacters(in: .whitespacesAndNewlines)
                } else if trimmedLine.contains("è¡¨è¾¾ï¼š") {
                    expressionType = trimmedLine.replacingOccurrences(of: "- è¡¨è¾¾ï¼š", with: "")
                        .replacingOccurrences(of: "è¡¨è¾¾ï¼š", with: "")
                        .trimmingCharacters(in: .whitespacesAndNewlines)
                } else if trimmedLine.contains("æƒ…æ„Ÿï¼š") {
                    emotion = trimmedLine.replacingOccurrences(of: "- æƒ…æ„Ÿï¼š", with: "")
                        .replacingOccurrences(of: "æƒ…æ„Ÿï¼š", with: "")
                        .trimmingCharacters(in: .whitespacesAndNewlines)
                } else if trimmedLine.contains("ç‰¹å¾ï¼š") {
                    characteristics = trimmedLine.replacingOccurrences(of: "- ç‰¹å¾ï¼š", with: "")
                        .replacingOccurrences(of: "ç‰¹å¾ï¼š", with: "")
                        .trimmingCharacters(in: .whitespacesAndNewlines)
                }
            }
            
            // æ ¹æ®è¡¨è¾¾æ–¹å¼ç¡®å®šéŸ³é¢‘ç±»å‹
            let audioType = inferAudioTypeFromExpression(expressionType, emotion: emotion)
            
            // æ„å»ºå¢å¼ºçš„æ€»ç»“ï¼ŒåŒ…å«å‰¯è¯­è¨€ä¿¡æ¯
            let enhancedSummary = buildEnhancedSummary(
                summary: summary,
                expression: expressionType, 
                emotion: emotion,
                characteristics: characteristics
            )
            
            print("ğŸ¯ å‰¯è¯­è¨€ä¿¡æ¯è§£æ - æ–¹å¼:\(expressionType) æƒ…æ„Ÿ:\(emotion) ç‰¹å¾:\(characteristics)")
            return (enhancedSummary, audioType)
            
        } else if response.contains("ç¯å¢ƒéŸ³:") {
            // ç¯å¢ƒéŸ³è§£æ
            var soundType = "ç¯å¢ƒå£°"
            var characteristics = ""
            
            let lines = response.components(separatedBy: "\n")
            for line in lines {
                let trimmedLine = line.trimmingCharacters(in: .whitespacesAndNewlines)
                
                if trimmedLine.contains("å£°éŸ³ï¼š") {
                    soundType = trimmedLine.replacingOccurrences(of: "- å£°éŸ³ï¼š", with: "")
                        .replacingOccurrences(of: "å£°éŸ³ï¼š", with: "")
                        .trimmingCharacters(in: .whitespacesAndNewlines)
                } else if trimmedLine.contains("ç‰¹å¾ï¼š") {
                    characteristics = trimmedLine.replacingOccurrences(of: "- ç‰¹å¾ï¼š", with: "")
                        .replacingOccurrences(of: "ç‰¹å¾ï¼š", with: "")
                        .trimmingCharacters(in: .whitespacesAndNewlines)
                }
            }
            
            let audioType = classifyEnvironmentSound(soundType)
            let enhancedSoundDescription = characteristics.isEmpty ? soundType : "\(soundType)(\(characteristics))"
            
            return (enhancedSoundDescription, audioType)
            
        } else if response.contains("æ€»ç»“:") || response.contains("æ€»ç»“ï¼š") {
            // å…¼å®¹æ—§æ ¼å¼
            let summary = response.replacingOccurrences(of: "æ€»ç»“:", with: "")
                .replacingOccurrences(of: "æ€»ç»“ï¼š", with: "")
                .trimmingCharacters(in: .whitespacesAndNewlines)
            let audioType = inferAudioType(from: response)
            return (summary, audioType)
            
        } else if response.contains("å£°éŸ³:") || response.contains("å£°éŸ³ï¼š") {
            // å…¼å®¹æ—§æ ¼å¼
            let soundType = response.replacingOccurrences(of: "å£°éŸ³:", with: "")
                .replacingOccurrences(of: "å£°éŸ³ï¼š", with: "")
                .trimmingCharacters(in: .whitespacesAndNewlines)
            let audioType = classifyEnvironmentSound(soundType)
            return (soundType, audioType)
            
        } else {
            // é€šç”¨å¤„ç†ï¼šæå–å…³é”®ä¿¡æ¯ä½œä¸ºæ€»ç»“
            print("ğŸ“‹ ä½¿ç”¨é€šç”¨è§£ææ ¼å¼")
            let cleanResponse = response.trimmingCharacters(in: .whitespacesAndNewlines)
            
            // ğŸ”§ æ”¹è¿›ï¼šæ›´æ™ºèƒ½çš„éŸ³é¢‘ç±»å‹åˆ¤æ–­
            let lowercasedResponse = response.lowercased()
            var audioType = AudioType.humanVoice
            
            // åªæœ‰å½“AIæ˜ç¡®è¯´è¿™æ˜¯å”±æ­Œæˆ–éŸ³ä¹æ—¶æ‰åˆ¤æ–­ä¸ºéŸ³ä¹
            // é¿å…å› ä¸ºæåˆ°"éŸ³ä¹"è¿™ä¸ªè¯å°±è¯¯åˆ¤
            if (lowercasedResponse.hasPrefix("å”±") || lowercasedResponse.hasPrefix("æ­Œ") ||
                lowercasedResponse.hasPrefix("éŸ³ä¹") || lowercasedResponse.hasPrefix("ğŸµ") ||
                lowercasedResponse.hasPrefix("ğŸ¶") || lowercasedResponse.hasPrefix("ğŸ¤")) {
                audioType = .singing
                print("ğŸµ æ£€æµ‹åˆ°éŸ³ä¹/æ­Œå”±å†…å®¹")
            } else if lowercasedResponse.contains("å”±") && lowercasedResponse.count < 10 {
                // çŸ­å›å¤ä¸­åŒ…å«"å”±"å­—ï¼Œå¯èƒ½æ˜¯éŸ³ä¹
                audioType = .singing
                print("ğŸµ çŸ­å›å¤æ£€æµ‹åˆ°å”±æ­Œå…³é”®è¯")
            } else {
                // é»˜è®¤ä¸ºäººå£°ï¼Œé™¤éæœ‰æ˜ç¡®çš„å…¶ä»–ç±»å‹æ ‡è®°
                audioType = .humanVoice
                print("ğŸ—£ï¸ é»˜è®¤åˆ¤æ–­ä¸ºäººå£°å†…å®¹")
            }
            
            let summary = extractSummaryFromResponse(cleanResponse)
            print("ğŸ“‹ é€šç”¨è§£æç»“æœ: æ€»ç»“=\(summary), ç±»å‹=\(audioType.displayName)")
            return (summary, audioType)
        }
    }
    
    // Extract meaningful summary from AI response
    private func extractSummaryFromResponse(_ response: String) -> String {
        // If response starts with emoji, keep entire response up to EOT marker
        if response.unicodeScalars.first?.properties.isEmoji == true {
            // Remove EOT marker and clean up
            let cleaned = response.replacingOccurrences(of: "<|EOT|>", with: "")
                                 .trimmingCharacters(in: .whitespacesAndNewlines)
            return cleaned
        }
        
        // Look for key content patterns
        if response.contains("ç”·æ€§") || response.contains("å¥³æ€§") {
            if response.contains("è¯´è¯") || response.contains("å¯¹è¯") {
                return "äººå£°å¯¹è¯"
            } else {
                return "äººå£°å†…å®¹"
            }
        }
        
        if response.contains("éŸ³ä¹") || response.contains("æ­Œæ›²") {
            return "éŸ³ä¹å†…å®¹"
        }
        
        if response.contains("å™ªéŸ³") || response.contains("æ‚éŸ³") {
            return "ç¯å¢ƒå™ªéŸ³"
        }
        
        // For responses with + symbols (like "å¨ç¬‘+é‚£ä¸ª+å¥å­"), keep the full content
        if response.contains("+") {
            let cleaned = response.replacingOccurrences(of: "<|EOT|>", with: "")
                                 .trimmingCharacters(in: .whitespacesAndNewlines)
            // Return up to first 20 characters to preserve meaning
            return String(cleaned.prefix(20))
        }
        
        // Extract first meaningful phrase
        let words = response.components(separatedBy: CharacterSet.whitespacesAndNewlines)
        for word in words {
            if word.count >= 2 && word.count <= 12 {
                return word
            }
        }
        
        // Fallback - keep more content
        return String(response.prefix(12))
    }
    
    // æ ¹æ®emojiå’Œå†…å®¹æ¨æ–­éŸ³é¢‘ç±»å‹
    private func inferAudioTypeFromEmojiContent(emoji: String, content: String) -> AudioType {
        let contentLower = content.lowercased()
        
        // åŸºäºemojiä¼˜å…ˆåˆ¤æ–­
        switch emoji {
        case "ğŸµ":
            return .singing
        case "ğŸ¢", "ğŸ“":
            return .conversation  
        case "ğŸ“¢":
            return .humanVoice
        case "ğŸ“š", "ğŸ“–":
            return .humanVoice
        case "ğŸ¤«":
            return .humanVoice
        case "ğŸŒ§ï¸", "ğŸŒªï¸":
            return .nature
        case "ğŸ˜Š", "ğŸ˜ ", "ğŸ˜¡", "ğŸ¤§", "ğŸ˜¤", "ğŸ˜®", "ğŸ˜¯", "ğŸ˜²", "ğŸ˜±", "ğŸ¤”", "ğŸ˜", "ğŸ¥º", "ğŸ˜­", "ğŸ¤­", "ğŸ™„":
            return .conversation
        case "ğŸ¤", "ğŸ™ï¸":
            return .humanVoice
        case "ğŸŒŠ", "ğŸŒ¨ï¸", "â„ï¸", "â˜€ï¸":
            return .nature
        case "ğŸ”§", "âš™ï¸", "ğŸš—", "âœˆï¸":
            return .mechanical
        default:
            break
        }
        
        // åŸºäºå†…å®¹å…³é”®è¯åˆ¤æ–­
        if contentLower.contains("å”±") || contentLower.contains("æ­Œ") {
            return .singing
        } else if contentLower.contains("ä¼šè®®") || contentLower.contains("å¯¹è¯") || contentLower.contains("èŠå¤©") {
            return .conversation
        } else if contentLower.contains("æ¼”è®²") || contentLower.contains("å‘è¨€") || contentLower.contains("æœ—è¯»") {
            return .humanVoice
        } else if contentLower.contains("é›¨") || contentLower.contains("é£") || contentLower.contains("é›·") {
            return .nature
        } else if contentLower.contains("éŸ³ä¹") || contentLower.contains("ä¹å™¨") {
            return .music
        } else {
            return .humanVoice // é»˜è®¤äººå£°
        }
    }
    
    // æ ¹æ®çŠ¶æ€å†…å®¹æ¨æ–­éŸ³é¢‘ç±»å‹
    private func inferAudioTypeFromStatus(_ statusContent: String) -> AudioType {
        let lowercased = statusContent.lowercased()
        
        // é€šè¿‡emojiå’Œå…³é”®è¯æ™ºèƒ½è¯†åˆ«
        if statusContent.contains("ğŸµ") || lowercased.contains("å”±") || lowercased.contains("æ­Œ") {
            return .singing
        } else if statusContent.contains("ğŸ“¢") || lowercased.contains("æ¼”è®²") || lowercased.contains("å‘è¨€") {
            return .humanVoice
        } else if statusContent.contains("ğŸ“–") || lowercased.contains("æœ—è¯»") || lowercased.contains("æœ—è¯µ") {
            return .humanVoice
        } else if statusContent.contains("ğŸ¤«") || lowercased.contains("è€³è¯­") || lowercased.contains("ç§è¯­") {
            return .humanVoice
        } else if lowercased.contains("ä¼šè®®") || lowercased.contains("å¯¹è¯") || lowercased.contains("äº¤è°ˆ") {
            return .conversation
        } else if lowercased.contains("é›¨å£°") || lowercased.contains("é£å£°") || lowercased.contains("é›·å£°") ||
                  lowercased.contains("æµ·æµª") || lowercased.contains("æµæ°´") || lowercased.contains("é¸Ÿ") {
            return .nature
        } else if lowercased.contains("éŸ³ä¹") || lowercased.contains("ä¹å™¨") {
            return .music
        } else if lowercased.contains("æœºå™¨") || lowercased.contains("è®¾å¤‡") || lowercased.contains("å¼•æ“") {
            return .mechanical
        } else if lowercased.contains("å™ª") || lowercased.contains("æ‚") {
            return .noise
        } else {
            return .humanVoice // é»˜è®¤äººå£°
        }
    }
    
    // æ ¹æ®è¡¨è¾¾æ–¹å¼å’Œæƒ…æ„Ÿæ¨æ–­éŸ³é¢‘ç±»å‹
    private func inferAudioTypeFromExpression(_ expression: String, emotion: String) -> AudioType {
        let expressionLower = expression.lowercased()
        
        if expressionLower.contains("å”±æ­Œ") || expressionLower.contains("æ­Œå”±") || expressionLower.contains("åŸå”±") {
            return .singing
        } else if expressionLower.contains("å¯¹è¯") || expressionLower.contains("äº¤è°ˆ") || expressionLower.contains("èŠå¤©") {
            return .conversation
        } else if expressionLower.contains("æ¼”è®²") || expressionLower.contains("æœ—è¯»") || expressionLower.contains("æœ—è¯µ") {
            return .humanVoice
        } else if expressionLower.contains("å‘¼å–Š") || expressionLower.contains("å«å–Š") {
            return .humanVoice
        } else if expressionLower.contains("è€³è¯­") || expressionLower.contains("çªƒçªƒç§è¯­") {
            return .humanVoice
        } else {
            return .humanVoice
        }
    }
    
    // æ„å»ºå¢å¼ºçš„æ€»ç»“ï¼ŒåŒ…å«å‰¯è¯­è¨€ä¿¡æ¯
    private func buildEnhancedSummary(summary: String, expression: String, emotion: String, characteristics: String) -> String {
        var enhancedSummary = summary
        
        // ç§»é™¤emojiå‰ç¼€ï¼Œä¿æŒæ‘˜è¦ç®€æ´
        // ç‰¹æ®Šè¡¨è¾¾æ–¹å¼çš„ä¿¡æ¯å·²ç»åœ¨åˆ†ç±»ä¸­ä½“ç°ï¼Œä¸éœ€è¦é¢å¤–æ ‡è¯†
        
        // ç§»é™¤æƒ…æ„Ÿemojiï¼Œä¿æŒæ–‡æœ¬ç®€æ´
        
        return enhancedSummary
    }
    
    private func inferAudioType(from content: String) -> AudioType {
        let lowercased = content.lowercased()
        
        if lowercased.contains("å¯¹è¯") || lowercased.contains("äº¤è°ˆ") {
            return .conversation
        } else if lowercased.contains("æ­Œ") || lowercased.contains("å”±") {
            return .singing
        } else if lowercased.contains("éŸ³ä¹") {
            return .music
        } else if lowercased.contains("æœºå™¨") || lowercased.contains("è®¾å¤‡") {
            return .mechanical
        } else if lowercased.contains("è‡ªç„¶") || lowercased.contains("é£") || lowercased.contains("é›¨") {
            return .nature
        } else if lowercased.contains("å™ªéŸ³") || lowercased.contains("åµ") {
            return .noise
        } else {
            return .humanVoice
        }
    }
    
    private func classifyEnvironmentSound(_ soundDescription: String) -> AudioType {
        let lowercased = soundDescription.lowercased()
        
        // éŸ³ä¹ç±»
        if lowercased.contains("éŸ³ä¹") || lowercased.contains("æ­Œ") || lowercased.contains("ä¹å™¨") || lowercased.contains("é’¢ç´") || lowercased.contains("å‰ä»–") {
            return .music
        }
        
        // è‡ªç„¶ç¯å¢ƒç±» - æ™ºèƒ½è¯†åˆ«å„ç§è‡ªç„¶å£°éŸ³
        else if lowercased.contains("é£") || lowercased.contains("é›¨") || lowercased.contains("é›ª") || lowercased.contains("é›·") ||
                lowercased.contains("æµ·æµª") || lowercased.contains("æµæ°´") || lowercased.contains("æºªæ°´") || lowercased.contains("ç€‘å¸ƒ") ||
                lowercased.contains("é¸Ÿ") || lowercased.contains("è™«") || lowercased.contains("è›™") || lowercased.contains("åŠ¨ç‰©") ||
                lowercased.contains("æ ‘å¶") || lowercased.contains("è‡ªç„¶") || lowercased.contains("æ£®æ—") || lowercased.contains("æµ·æ´‹") {
            return .nature
        }
        
        // æœºæ¢°è®¾å¤‡ç±»
        else if lowercased.contains("æœº") || lowercased.contains("ç”µ") || lowercased.contains("è½¦") || lowercased.contains("å¼•æ“") ||
                lowercased.contains("è®¾å¤‡") || lowercased.contains("å·¥å…·") || lowercased.contains("é©¬è¾¾") || lowercased.contains("ç©ºè°ƒ") {
            return .mechanical
        }
        
        // å™ªéŸ³æ‚éŸ³ç±»
        else if lowercased.contains("å™ª") || lowercased.contains("æ‚") || lowercased.contains("å˜ˆ") || lowercased.contains("åµ") {
            return .noise
        }
        
        // é»˜è®¤åˆ†ç±» - è®©æ¨¡å‹çš„æ™ºèƒ½è¯†åˆ«ç»“æœå†³å®š
        else {
            // æ ¹æ®æè¿°å†…å®¹æ™ºèƒ½æ¨æ–­
            if soundDescription.count > 2 {
                return .unknown // ä¿æŒä¸ºunknownï¼Œè®©UIæ˜¾ç¤ºåŸå§‹æè¿°
            } else {
                return .noise
            }
        }
    }
}

// MARK: - URLSessionWebSocketDelegate

extension StepRealtimeManager: URLSessionWebSocketDelegate {
    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didOpenWithProtocol protocol: String?) {
        print("âœ… WebSocketè¿æ¥å·²å»ºç«‹")
        
        updateConnectionStatus(.connected)
        resetReconnectAttempts()
        
        // Reset failure tracking on successful connection
        consecutiveFailures = 0
        circuitBreakerOpen = false
        lastConnectionFailure = nil
        
        // Start ping timer for this connection
        startPingTimer(for: webSocketTask)
        
        // Send session configuration immediately after connection
        Task {
            await configureSession()
        }
    }
    
    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didCloseWith closeCode: URLSessionWebSocketTask.CloseCode, reason: Data?) {
        let reasonString = reason.flatMap { String(data: $0, encoding: .utf8) } ?? "No reason provided"
        print("ğŸ”Œ WebSocketè¿æ¥å·²å…³é—­ - Code: \(closeCode.rawValue), Reason: \(reasonString)")
        
        DispatchQueue.main.async {
            self.isRecording = false
            self.isProcessing = false
            self.currentTranscription = ""
            self.currentSummary = ""
        }
        
        // Handle disconnection based on close code
        if !isManualDisconnect {
            if closeCode == .normalClosure {
                print("ğŸ“ Normal WebSocket closure")
                updateConnectionStatus(.disconnected)
            } else {
                print("âš ï¸ Unexpected WebSocket closure, attempting reconnect")
                handleConnectionFailure()
            }
        }
    }
    
    func urlSession(_ session: URLSession, task: URLSessionTask, didCompleteWithError error: Error?) {
        if let error = error {
            print("âŒ WebSocketä»»åŠ¡å®Œæˆæ—¶å‡ºé”™: \(error.localizedDescription)")
            if let urlError = error as? URLError {
                print("âŒ URLé”™è¯¯ä»£ç : \(urlError.code.rawValue)")
                print("âŒ é”™è¯¯è¯¦æƒ…: \(urlError.localizedDescription)")
                
                // Handle specific error codes
                switch urlError.code {
                case .networkConnectionLost, .timedOut, .notConnectedToInternet:
                    print("ğŸŒ Network connectivity issue, will retry")
                    if !isManualDisconnect {
                        handleConnectionFailure()
                    }
                default:
                    break
                }
            }
        }
    }
}

// MARK: - æ•°æ®æ¨¡å‹

struct ConversationItem: Identifiable {
    let id = UUID()
    let transcription: String
    let summary: String
    let timestamp: Date
    let audioType: String
    
    init(transcription: String, summary: String, timestamp: Date = Date(), audioType: String = "unknown") {
        self.transcription = transcription
        self.summary = summary
        self.timestamp = timestamp
        self.audioType = audioType
    }
}